
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 9: Information Theory &#8212; Unit Reader</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/week09/L_InformationTheory';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Week 10: Game theory" href="../week10/L_Game_theory.html" />
    <link rel="prev" title="Week 8: Critical behaviour" href="../week08/L_Critical_phenomena.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Unit Reader</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    MATH3024 Reader
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../week01/L_Introduction_to_complex_systems.html">Week 1: Welcome to MATH3024: (modelling) Complex Systems</a></li>





<li class="toctree-l1"><a class="reference internal" href="../week02/L_Fractals.html">Week 2: Fractals</a></li>






<li class="toctree-l1"><a class="reference internal" href="../week03/L_Reaction_diffusion.html">Week 3: Reaction-diffusion systems</a></li>











<li class="toctree-l1"><a class="reference internal" href="../week04/L_Cellular_automata.html">Week 4: Cellular automata</a></li>









<li class="toctree-l1"><a class="reference internal" href="../week05/L_ABM.html">Week 5: Agent-based modelling</a></li>







<li class="toctree-l1"><a class="reference internal" href="../week06/L_Synchronisation.html">Week 6: Synchronisation</a></li>






<li class="toctree-l1"><a class="reference internal" href="../week07/L_Intelligent_systems.html">Week 7: Intelligent systems</a></li>








<li class="toctree-l1"><a class="reference internal" href="../week08/L_Critical_phenomena.html">Week 8: Critical behaviour</a></li>





<li class="toctree-l1 current active"><a class="current reference internal" href="#">Week 9: Information Theory</a></li>







<li class="toctree-l1"><a class="reference internal" href="../week10/L_Game_theory.html">Week 10: Game theory</a></li>



</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/notebooks/week09/L_InformationTheory.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 9: Information Theory</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Week 9: Information Theory</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#what-exactly-is-complexity">WHAT exactly is ‘complexity’?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#where-is-complexity">WHERE is ‘complexity’?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-want-to-measure-complexity">WHY do we want to measure ‘complexity’?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-measure-complexity">HOW do we measure ‘complexity’?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#who-and-when-a-brief-origin-story">WHO and WHEN: a brief origin story</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#information">Information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy">Entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-as-expected-surprise">Entropy as expected surprise</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recall-expected-value">Recall: Expected value</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shannon-entropy-h-x">Shannon Entropy, <span class="math notranslate nohighlight">\(H(X)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing">Comparing</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summarising">Summarising</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#absolute-meaning">Absolute meaning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#relative-meaning">Relative meaning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-in-practice">Interpretation in practice</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-entropy-h-x-y">Joint entropy, <span class="math notranslate nohighlight">\(H(X,Y)\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-entropy-h-x-y">Conditional entropy, <span class="math notranslate nohighlight">\(H(X|Y)\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mutual-information-i-x-y">Mutual information, <span class="math notranslate nohighlight">\(I(X; Y)\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#many-others-too">Many others too</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting it all together</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#applications">Applications</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-series-analysis">Time series analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#permutation-entropy">Permutation Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-entropy">Transfer Entropy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-in-general">Data in general</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="week-9-information-theory">
<h1>Week 9: Information Theory<a class="headerlink" href="#week-9-information-theory" title="Link to this heading">#</a></h1>
<p><img alt="" src="../../_images/ChatGPT_Complexity.png" /></p>
<p>We will now come full circle to try understand one of the main questions that has run through everything we’ve covered since day one…</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="what-exactly-is-complexity">
<h1>WHAT exactly is ‘complexity’?<a class="headerlink" href="#what-exactly-is-complexity" title="Link to this heading">#</a></h1>
<p>We are trying to answer this because we don’t have a precise definition of what a complex system is (new field finding its feet, multi-disciplinary with very large scope etc etc). All we really have is a list of properties that we require these systems to have (and even this isn’t conclusive as they aren’t all required all the time).</p>
<p>Before we tackle this question let’s answer some easier ones…</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="where-is-complexity">
<h1>WHERE is ‘complexity’?<a class="headerlink" href="#where-is-complexity" title="Link to this heading">#</a></h1>
<p>By now, I hope that it is clear that <strong>complex systems are everywhere</strong>.</p>
<p>They appear in the natural world:</p>
<ul class="simple">
<li><p>ecosystems, food webs, and climate,</p></li>
<li><p>the brain, immune system, and genetic regulation,</p></li>
<li><p>fractals and turbulence in physical systems.</p></li>
</ul>
<p>They appear in human society:</p>
<ul class="simple">
<li><p>economies and financial markets,</p></li>
<li><p>cities, transport, and communication networks,</p></li>
<li><p>cultural diffusion, cooperation, and conflict.</p></li>
</ul>
<p>They appear in technology:</p>
<ul class="simple">
<li><p>the internet, power grids, and social media,</p></li>
<li><p>artificial intelligence and multi-agent systems.</p></li>
</ul>
<p>Complex systems are unpredictable. Wherever there are many interacting parts giving rise to emergent patterns, we find complexity.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="why-do-we-want-to-measure-complexity">
<h1>WHY do we want to measure ‘complexity’?<a class="headerlink" href="#why-do-we-want-to-measure-complexity" title="Link to this heading">#</a></h1>
<p>The defining scientific challenges of the 21st century — climate change, pandemics, disinformation, economic inequality — all share something in common. They are <strong>emergent problems</strong> of complex systems: unpredictable outcomes from countless interactions among individuals, institutions, and technologies.</p>
<p>Complex systems often display behaviours like synchronisation, critical phase changes, cascading failures, heavy-tailed events, and self-organisation. We already have tools to model such behaviour and now seek to better quantify it.</p>
<p>By measuring complexity we can move beyond description to characterise and compare systems:</p>
<ul class="simple">
<li><p><strong>compare systems or models</strong>: judge whether one is “more complex” than another; useful for testing alternatives or contrasting natural vs. artificial systems.</p></li>
<li><p><strong>track behaviour and transitions</strong>: complexity changes over time and signals shifts from order to chaos or tipping points.</p>
<ul>
<li><p>ecosystems: rising complexity may signal diversification; collapse shows loss of complexity.</p></li>
<li><p>physics/ABMs: changes in correlation length or entropy may reveal criticality.</p></li>
</ul>
</li>
<li><p><strong>evaluate model fidelity</strong>: does the model capture the richness of real dynamics, not just averages?</p></li>
<li><p><strong>quantify information content</strong>: how much information is encoded in structure or dynamics (this week’s focus).</p></li>
<li><p><strong>aid prediction and control</strong>: between simplicity and randomness lies the “edge of chaos” where adaptation and learning are possible.</p></li>
<li><p><strong>communicate across disciplines</strong>: complexity measures offer a <em>common language</em> across biology, physics, sociology, economics, and beyond.</p></li>
</ul>
<div style="margin: 1.5em 0; border-right: 4px solid #888; padding-right: 1em; text-align: right;">
  <div style="font-style: italic; font-family: 'Comic Neue', 'Segoe Script', cursive; font-size: 1.2em;">
    [Information theory is] poised to become the lingua franca of our increasingly complexity-aware scientific enterprise  </div>
  <div style="font-style: normal; font-size: 0.95em; margin-top: 0.5em;">
    — Information Theory for Complex Systems Scientists: What, Why, & How?, Thomas F. Varley
  </div>
</div></section>
<section class="tex2jax_ignore mathjax_ignore" id="how-do-we-measure-complexity">
<h1>HOW do we measure ‘complexity’?<a class="headerlink" href="#how-do-we-measure-complexity" title="Link to this heading">#</a></h1>
<p>How do we give a climate scientist and a sociologist, or a neuroscientist and an economist, a <em>common language</em> so that they can compare complexity across disciplines?</p>
<p>We have already seen one measure of complexity…</p>
<p><strong>Fractal dimension</strong>.</p>
<p>A larger fractal dimension means the object fills space more densely, capturing self-similarity across scales. e.g.:</p>
<ul class="simple">
<li><p>the Koch curve has fractal dimension <span class="math notranslate nohighlight">\(\approx 1.26\)</span>: more complex than a straight line (dimension 1) but far from filling a plane.</p></li>
<li><p>the Peano curve has fractal dimension 2, fully covering the plane.</p></li>
</ul>
<p>We have also used order parameters (e.g. magnetisation/alignment in the Ising/Vicsek model, synchronisation in Kuramoto oscillators). While not always labelled as complexity measures, they serve a similar purpose: reducing high-dimensional behaviour to a single number that tracks emergent organisation.</p>
<p>These measures are valuable but limited. We need something that can capture not just structure or organisation, but also the unpredictability and variability of system behaviour.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="who-and-when-a-brief-origin-story">
<h1>WHO and WHEN: a brief origin story<a class="headerlink" href="#who-and-when-a-brief-origin-story" title="Link to this heading">#</a></h1>
<p>Information theory was born in 1948, when Claude Shannon, working at Bell Labs, tackled a practical engineering problem: <em>How can a message be transmitted reliably over a noisy channel?</em></p>
<p><img alt="" src="notebooks/week09/images/Claude_Shannon.jpg" /></p>
<p>From this question, Shannon developed the mathematics of information: how to quantify, encode, and transmit it efficiently. His framework laid the foundation for digital communication, computing, and data science, and has been described as <em>“on par with electricity or the internal combustion engine in terms of impact.”</em></p>
<p>Although Shannon’s immediate focus was telegraphs and phone lines, his theory of measuring uncertainty, structure, and order proved far more general. It has since influenced physics, biology, mathematics, computer science, and philosophy.</p>
<p>Today, information theory underpins applications such as:</p>
<ul class="simple">
<li><p>how your phone compresses photos,</p></li>
<li><p>quantifying genetic information in DNA,</p></li>
<li><p>measuring correlations in neural networks,</p></li>
<li><p>detecting critical transitions in ecosystems,</p></li>
<li><p>modelling the spread of ideas and disinformation in society.</p></li>
</ul>
<p>Information theory is a toolkit to quantify complexity itself.</p>
<p>We will look extensively at entropy as the key measure.</p>
<ol class="arabic simple">
<li><p><strong>Entropy</strong>: measures uncertainty and disorder in a system.</p></li>
</ol>
<p>And then consider some variants:</p>
<ol class="arabic simple" start="2">
<li><p><strong>Joint entropy</strong>: quantifies the total uncertainty of two (or more) variables taken together.</p></li>
<li><p><strong>Conditional entropy</strong>: measures the remaining uncertainty in one variable given knowledge of another.</p></li>
<li><p><strong>Mutual information</strong>: reveals hidden dependencies, capturing how much knowing one variable reduces uncertainty about another.</p></li>
</ol>
<p>These quantities are universal. They apply whether we’re analysing spins in the Ising model, birds in a flock, or rumours on social media.</p>
<p>To summarise the roadmap for this topic:</p>
<p>We need a way to <em>quantify uncertainty</em> in outcomes of complex systems.</p>
<ul class="simple">
<li><p>information reduces it,</p></li>
<li><p>entropy quantifies it,</p></li>
<li><p>and the variants let us capture relationships.</p></li>
</ul>
<section id="information">
<h2>Information<a class="headerlink" href="#information" title="Link to this heading">#</a></h2>
<p>Information is what reduces our uncertainty when data is revealed.</p>
<p>To Shannon, “information” referred to signals transmitted through a channel.</p>
<p>Intuitively, information measures <em>how much our uncertainty is reduced when new data becomes available</em>.</p>
<p>The problem of transmitting a message is just one special case of this general framework. The same mathematics applies whenever we seek to reduce uncertainty about an unknown variable, such as:</p>
<ul class="simple">
<li><p>tracking information flow,</p></li>
<li><p>predicting the future state of a system,</p></li>
<li><p>measuring coupling between variables,</p></li>
<li><p>quantifying unpredictability.</p></li>
</ul>
<p>This is why information theory provides a natural framework for studying complex systems.</p>
<p>A really important point is that ‘information’ is agnostic to the question of the “meaning” of a message.</p>
<ul class="simple">
<li><p><strong>Information</strong>: a measure of the reduction in uncertainty about a system or event when a signal is received.</p></li>
<li><p><strong>Meaning</strong>: involves interpretation and context, which are subjective and vary depending on the receiver.</p></li>
</ul>
<p>i.e. We aren’t interested in what the information represents - only that we have inferred the correct message from some space of possible messages.</p>
<p>e.g. Consider the message:</p>
<div style="text-align: center; color: blue; font-family: 'Georgia', serif; font-size: 22px;">
<em>"The sky is blue."</em>
</div>
<p>In the context of Information Theory, this sentence is just a sequence of characters.</p>
<p>We care about:</p>
<ul class="simple">
<li><p>how many bits are needed to transmit the message?</p></li>
<li><p>what is the likelihood of errors in transmission?</p></li>
<li><p>how efficiently can the message be encoded and decoded?</p></li>
</ul>
<p>We don’t care about:</p>
<ul class="simple">
<li><p>what “sky” or “blue” mean</p></li>
<li><p>that this sentence conveys something about the weather</p></li>
</ul>
<p>These meanings arise in the mind of the receiver and involve interpretation, which is outside the scope of Information Theory.</p>
<p>Information Theory would treat both “The sky is blue” and some random characters “ajksd873##&#64;” in much the same way: as strings of characters.</p>
<p>The focus is on the quantity of information rather than its content or interpretation. In fact, the random string is considered as having more information content because it is less predictable.</p>
<p>At its core, it is the mathematics of inference (encoding, decoding, probability, and statistics) especially under conditions of uncertainty.</p>
<p>Shannon’s key insight was that messages aren’t random strings though. Some symbols/words are more probable than others. e.g.:</p>
<ul class="simple">
<li><p>In English text, <em>E</em> is much more likely than <em>Z</em>.</p></li>
<li><p>The digram <em>TH</em> is far more likely than <em>XZ</em>.</p></li>
<li><p>Whole words like “the” occur with very high frequency.</p></li>
</ul>
<p>So instead of assuming equal probabilities (each symbol would carry <span class="math notranslate nohighlight">\(\log_2 27 \approx 4.75\)</span> bits), he measured (or estimated) them from language statistics.</p>
<p>The actual entropy rate is only <span class="math notranslate nohighlight">\(\approx 1–1.5\)</span> bits/letter. This “gap” is <em>redundancy</em> in language, which is why compression and error correction are possible.</p>
</section>
<section id="entropy">
<h2>Entropy<a class="headerlink" href="#entropy" title="Link to this heading">#</a></h2>
<p>Because information is the reduction of uncertainty associated with observing data, we must first understand uncertainty.</p>
<p>This is done with entropy: a mathematical function that quantifies how uncertain we are (on average) about the state of some variable <em>before</em> any message is received.</p>
<p>There are many ways to define entropy; we will focus on the perspective of entropy as expected surprise.</p>
<p>Note that we are not discussing entropy from physics/thermodynamics, which you are likely familiar with.</p>
<p>Both measure the logarithm of possible states — but our emphasis is on information and complexity, not heat and energy.</p>
<section id="entropy-as-expected-surprise">
<h3>Entropy as expected surprise<a class="headerlink" href="#entropy-as-expected-surprise" title="Link to this heading">#</a></h3>
<p>Some events are more surprising than others.</p>
<p>How surprised you are depends on how unlikely the event was.</p>
<p>Consider a fair die:</p>
<ul class="simple">
<li><p>each face has probability of 1/6</p></li>
<li><p>no outcome is more surprising than another (if you were gambling on the die: no reason to pick any face over another)</p></li>
</ul>
<p><img alt="" src="../../_images/Die.png" /></p>
<p>Now consider a loaded die:</p>
<ul class="simple">
<li><p>suppose 2 comes up 2/3 of the time, all other faces come up with probability 1/15</p></li>
<li><p>knowing this probability distribution ahead of time, you will be less surprised to see a 2 come up than you would be to see a 5 come up.</p></li>
</ul>
<p><img alt="" src="../../_images/Die_Roman.jpg" /></p>
<p>So how do we quantify surprise?</p>
<p>We want a function for the surprise of an event <span class="math notranslate nohighlight">\(h(x)\)</span>, that satisfies:</p>
<ol class="arabic simple">
<li><p>more probable events are less surprising (<span class="math notranslate nohighlight">\(h(x)\propto 1/P(x)\)</span>)</p></li>
</ol>
<ol class="arabic simple" start="2">
<li><p>surprise is never be negative</p></li>
</ol>
<ol class="arabic simple" start="3">
<li><p>independent events are additive (<span class="math notranslate nohighlight">\(h(x_1,x_2)=h(x_1)+h(x_2)\)</span>)</p></li>
</ol>
<ol class="arabic simple" start="4">
<li><p>surprise varies continuously</p></li>
</ol>
<ol class="arabic simple" start="5">
<li><p>guaranteed events are unsurprising: <span class="math notranslate nohighlight">\(P(x) = 1 \implies h(x) = 0\)</span></p></li>
</ol>
<ol class="arabic simple" start="6">
<li><p>impossible events are infinitely surprising: <span class="math notranslate nohighlight">\(P(x)=0 \implies h(x)=\infty\)</span>.</p></li>
</ol>
<p>The unique function meeting these criteria is:</p>
<div class="math notranslate nohighlight">
\[
h(x)=-\log P(x).
\]</div>
<p>This is called <strong>surprise</strong>, <strong>self-information</strong> or the <strong>local entropy</strong>.</p>
<p>Having defined the surprise of a single outcome, we now ask: <em>What is the average surprise if we observe the system many times?</em></p>
<p>To answer this, we take the <strong>expected value of surprise</strong> across all possible outcomes.</p>
<p>This is the <strong>Shannon entropy</strong>.</p>
<section id="recall-expected-value">
<h4>Recall: Expected value<a class="headerlink" href="#recall-expected-value" title="Link to this heading">#</a></h4>
<p>The <strong>expected value</strong> of a random variable is computed by taking the sum of all possible outcomes, each weighted by its probability</p>
<p>For a discrete random variable <span class="math notranslate nohighlight">\(X\)</span>, the expected value <span class="math notranslate nohighlight">\(\mathbb{E}[X]\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[X] = \sum_{x} p(x) \cdot x
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(x)\)</span> is the probability of outcome <span class="math notranslate nohighlight">\(x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x\)</span> is the value of the outcome.</p></li>
</ul>
</section>
</section>
<section id="shannon-entropy-h-x">
<h3>Shannon Entropy, <span class="math notranslate nohighlight">\(H(X)\)</span><a class="headerlink" href="#shannon-entropy-h-x" title="Link to this heading">#</a></h3>
<p>Formally, entropy is the expected surprise over all outcomes:</p>
<div class="math notranslate nohighlight">
\[
H(X)=\mathbb{E}_X[h(X)]=-\sum_{x\in \mathcal{X}}P(x)\log P(x).
\]</div>
<p>Key ideas:</p>
<ul class="simple">
<li><p>rare events are very surprising but we aren’t that surprised very often</p></li>
<li><p>frequent events are unsurprising, so we spend a lot of time not being very surprised.</p></li>
<li><p>entropy balances these, giving the average uncertainty.</p></li>
</ul>
<p>Note that by convention we say that <span class="math notranslate nohighlight">\(0 \log(0) = 0\)</span>, so impossible events have no impact on our uncertainty.</p>
<p>Shannon entropy is <em>the</em> fundamental building block of Information Theory.</p>
<p>So which dice - the fair or the loaded one - has a higher entropy?</p>
<p><strong>Fair die</strong>: is maximally uncertain — all outcomes equally likely.</p>
<div class="math notranslate nohighlight">
\[
H(X) = -6\cdot\tfrac{1}{6}\log_2\tfrac{1}{6} = \log_2 6 \approx 2.59 \text{ bits.}
\]</div>
<p><strong>Loaded die</strong> (<span class="math notranslate nohighlight">\(P(2)=\tfrac{2}{3},\; P(\text{others})=\tfrac{1}{15}\)</span>): is more predictable (most of the time it’s “2”), so the average uncertainty is lower</p>
<div class="math notranslate nohighlight">
\[
H(X) = -\Big[\tfrac{2}{3}\log_2\tfrac{2}{3} + 5\cdot\tfrac{1}{15}\log_2\tfrac{1}{15}\Big] \approx 1.69 \text{ bits.}
\]</div>
<div style="border-left: 4px solid #1e70bf; padding: 0.75em; background-color: #eaf3fb; margin-bottom: 1em;">
  <strong>Test your understanding:</strong><br>
  Does rolling a die or tossing a coin have a higher entropy?
</div><div class="toggle docutils container">
<p>Rolling a die has higher entropy than tossing a coin.</p>
<p>Each outcome of a die toss has smaller probability (<span class="math notranslate nohighlight">\(p=1/6\)</span>) than each outcome of a coin toss ($p=1/2).</p>
</div>
<p>You might like to keep this picture for a fair coin toss in mind:</p>
<p>For a Bernoulli random variable with probabilities <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(1-p\)</span>:</p>
<div class="math notranslate nohighlight">
\[
H(p) = -p \log p - (1-p) \log (1-p).
\]</div>
<p><img alt="" src="../../_images/Binary_entropy_plot.png" /></p>
<p>Note that <span class="math notranslate nohighlight">\(-x \log x\)</span> (single-term contribution) is asymmetric, peaks at <span class="math notranslate nohighlight">\(1/e\)</span>.</p>
<p>In general entropy is a high-dimensional surface that lives on the <em>probability simplex</em>, which is the set of all valid probability distributions.</p>
<p><strong>Details</strong>:</p>
<p>For a system with <span class="math notranslate nohighlight">\(k\)</span> possible outcomes, we describe it with probabilities</p>
<div class="math notranslate nohighlight">
\[
(p_1, p_2, \dots, p_k), \quad \sum_{i=1}^k p_i = 1, \quad p_i \geq 0.
\]</div>
<ul class="simple">
<li><p>For a coin (<span class="math notranslate nohighlight">\(k=2\)</span>): the simplex is 1D (a line segment from <span class="math notranslate nohighlight">\((1,0)\)</span> to <span class="math notranslate nohighlight">\((0,1)\)</span>) (the parabola-like binary entropy).</p></li>
<li><p>For a die (<span class="math notranslate nohighlight">\(k=6\)</span>): the simplex is 5D.</p></li>
</ul>
<p>Entropy is defined by all <span class="math notranslate nohighlight">\(k\)</span> probabilities together.:</p>
<div class="math notranslate nohighlight">
\[
H(p_1,\dots,p_k) = -\sum_{i=1}^k p_i \log p_i.
\]</div>
<ul class="simple">
<li><p>This is a <strong>surface</strong> defined over the simplex.</p></li>
<li><p>At one corner of the simplex (all probability on a single outcome), entropy = 0.</p></li>
<li><p>At the centre of the simplex (all probabilities equal), entropy = <span class="math notranslate nohighlight">\(\log k\)</span>, the maximum.</p></li>
</ul>
</section>
<section id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Link to this heading">#</a></h3>
<p>Consider some known distributions…</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a normally distributed random variable, <span class="math notranslate nohighlight">\(X \sim N(\mu, \sigma^2)\)</span>, with probability density function (PDF) <span class="math notranslate nohighlight">\(f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
H(X) &amp;= -\int_{-\infty}^{\infty} f(x) \log f(x) \, dx \\
\therefore H_{\text{normal}} &amp;= -\int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right) \log \left(\frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)\right) \, dx \\
&amp;= -\int_{-\infty}^{\infty} f(x) \left(-\frac{1}{2} \log(2 \pi \sigma^2) - \frac{(x - \mu)^2}{2\sigma^2}\right) \, dx
\end{align*}
\end{split}\]</div>
<p>The first term (the constant <span class="math notranslate nohighlight">\(\frac{1}{2} \log(2 \pi \sigma^2) \)</span>) comes out of the integral and integrates to 1, and the second term integrates to give <span class="math notranslate nohighlight">\(\sigma^2 \)</span>. The result is:</p>
<div class="math notranslate nohighlight">
\[
H_{\text{normal}} = \frac{1}{2} \log(2 \pi e \sigma^2)
\]</div>
<p>Thus, the entropy of a normal distribution depends on the variance <span class="math notranslate nohighlight">\(\sigma^2 \)</span>. The larger the variance, the more spread out the distribution, and thus the higher the entropy.</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be an exponentially distributed random variable, <span class="math notranslate nohighlight">\(X \sim \text{Exp}(\lambda) \)</span> with probability density function (PDF), <span class="math notranslate nohighlight">\(f(x) = \lambda \exp(-\lambda x) \quad \text{for} \, x \geq 0\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
H(X) &amp;= -\int_{0}^{\infty} f(x) \log f(x) \, dx \\
\therefore H_{\text{exponential}} &amp;= -\int_{0}^{\infty} \lambda \exp(-\lambda x) \log(\lambda \exp(-\lambda x)) \, dx \\
&amp;= -\int_{0}^{\infty} \lambda \exp(-\lambda x) \left(\log(\lambda) - \lambda x\right) \, dx\\
&amp;= -\log(\lambda) \int_{0}^{\infty} \lambda \exp(-\lambda x) \, dx + \lambda \int_{0}^{\infty} \lambda x \exp(-\lambda x) \, dx
\end{align*}
\end{split}\]</div>
<p>The first integral evaluates to 1, and the second integral evaluates to <span class="math notranslate nohighlight">\(\frac{1}{\lambda}\)</span>. Hence,</p>
<div class="math notranslate nohighlight">
\[
H_{\text{exponential}} = -\log(\lambda) + 1
\]</div>
<p>Thus, the entropy of an exponential distribution depends on the rate parameter <span class="math notranslate nohighlight">\(\lambda\)</span>. The smaller the rate, the more spread out the distribution, and thus the higher the entropy.</p>
<section id="comparing">
<h4>Comparing<a class="headerlink" href="#comparing" title="Link to this heading">#</a></h4>
<p>The normal distribution’s entropy grows with the variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, while the exponential distribution’s entropy is inversely related to the rate <span class="math notranslate nohighlight">\(\lambda\)</span></p>
<p>For comparable scales (a similar spread/variance):</p>
<p>Normal distribution <span class="math notranslate nohighlight">\(N(\mu, \sigma^2)\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_{\text{normal}} = \frac{1}{2} \log(2 \pi e \sigma^2)\)</span></p></li>
<li><p>defined over the entire real line and spreads out more</p></li>
<li><p>generally has higher entropy</p></li>
</ul>
<p>Exponential distribution <span class="math notranslate nohighlight">\(\text{Exp}(\lambda)\)</span>:</p>
<ul class="simple">
<li><p>(Recall that the variance of the exponential distribution is <span class="math notranslate nohighlight">\(\frac{1}{\lambda^2}\)</span>.)</p></li>
<li><p><span class="math notranslate nohighlight">\(H_{\text{exponential}} = 1 - \log(\lambda)\)</span></p></li>
<li><p>despite its unbounded support, concentrates more heavily near 0 and decays rapidly</p></li>
<li><p>generally has lower entropy</p></li>
</ul>
<div style="border-left: 4px solid #1e70bf; padding: 0.75em; background-color: #eaf3fb; margin-bottom: 1em;">
  <strong>Pause and consider:</strong><br>
Order the behavioural regimes of a Couzin model in terms of their entropy.
</div><div class="toggle docutils container">
<p>We need to be clear about what variable the entropy is over.</p>
<p>If we measure entropy of heading angles at a snapshot <span class="math notranslate nohighlight">\(H(\Theta)\)</span>:</p>
<ul class="simple">
<li><p>Highest: Random (uniform over <span class="math notranslate nohighlight">\([0,2\pi)\)</span>) and also rotational milling (headings wrap uniformly around the ring)</p></li>
<li><p>Middle: Dynamic polarised (unimodal but broad around a mean)</p></li>
<li><p>Lowest: Highly polarised order (tight spike around one direction)</p></li>
</ul>
<p>To capture the difference between the random state and the ordered milling we would need to also consider the spatial structure, say with <span class="math notranslate nohighlight">\(H(X, \Theta)\)</span>. Or perhaps we could instead consider something like relative angles locally?</p>
</div>
</section>
</section>
<section id="summarising">
<h3>Summarising<a class="headerlink" href="#summarising" title="Link to this heading">#</a></h3>
<p>The question we set out to answer was “WHAT is complexity”</p>
<p>We have answered a different question: How uncertain is the system.</p>
<section id="absolute-meaning">
<h4>Absolute meaning<a class="headerlink" href="#absolute-meaning" title="Link to this heading">#</a></h4>
<p>Entropy is measured in <strong>bits (or nats)</strong>, and it tells you:</p>
<ul class="simple">
<li><p>the <em>average surprise per symbol</em>, or</p></li>
<li><p>the <em>average number of yes/no questions</em> needed to identify an outcome.</p></li>
</ul>
<p>So if you calculate <span class="math notranslate nohighlight">\(H(X) = 2.59\)</span> bits for a fair die, that literally means: on average, you need <span class="math notranslate nohighlight">\(\approx 2.59\)</span> binary questions to specify the face.</p>
</section>
<section id="relative-meaning">
<h4>Relative meaning<a class="headerlink" href="#relative-meaning" title="Link to this heading">#</a></h4>
<p>On its own, the <em>magnitude</em> of entropy is less useful unless you compare it to something, like:</p>
<ul class="simple">
<li><p><strong>Maximum entropy</strong>: For <span class="math notranslate nohighlight">\(k\)</span> equally likely outcomes, the maximum possible entropy is <span class="math notranslate nohighlight">\(\log_2 k\)</span>.</p></li>
<li><p><strong>Normalisation</strong>: You often look at entropy relative to this maximum (so you can say a distribution is “60% of maximum uncertainty”).</p></li>
<li><p><strong>Comparisons across systems</strong>: Entropy lets you compare two models, two distributions or even to the same system under different conditions, in a principled way.</p></li>
</ul>
</section>
<section id="interpretation-in-practice">
<h4>Interpretation in practice<a class="headerlink" href="#interpretation-in-practice" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>low</strong> <span class="math notranslate nohighlight">\(H\)</span>: outcomes are predictable, the system has order.</p></li>
<li><p><strong>high</strong> <span class="math notranslate nohighlight">\(H\)</span>: outcomes are uncertain, the system is closer to randomness.</p></li>
<li><p>If you just quote “<span class="math notranslate nohighlight">\(H=1.7\)</span> bits,” the number alone is meaningless unless you also know: <em>“out of how many possible outcomes?”</em></p></li>
</ul>
<p>Entropy gives us a rigorous measure of uncertainty — the average surprise in outcomes — but that is not the same as a measure of complexity. Instead, it provides the toolkit we need to measure the uncertainty, dependencies, and information flows that underlie complex behaviour.</p>
<p>And so unfortunately we don’t have a value for complexity.</p>
<p>By extending entropy to compare variables (through conditional entropy, joint entropy, and mutual information) we can begin to capture relationships and structure that move us closer to the idea of complexity…</p>
<p>These are essential ingredients of complex systems: emergence, adaptation, and interaction all hinge on flows of information and uncertainty.</p>
</section>
</section>
</section>
<section id="joint-entropy-h-x-y">
<h2>Joint entropy, <span class="math notranslate nohighlight">\(H(X,Y)\)</span><a class="headerlink" href="#joint-entropy-h-x-y" title="Link to this heading">#</a></h2>
<p>Measures the total uncertainty in two random variables considered together.</p>
<ul class="simple">
<li><p>Reflects the uncertainty of each variables <em>and</em> their interactions.</p></li>
<li><p>Useful for understanding interdependencies: higher dependence means lower joint entropy (relative to the independent case).</p></li>
</ul>
<p>Formally defined as the union of both marginal entropies:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H(X,Y)=\mathbb{E}_{X,Y}[h(X,Y)]=-\sum_{x\in \mathcal{X}\\ y\in \mathcal{Y}}P(x,y)\log P(x,y).
\end{split}\]</div>
<p>Joint and marginal entropies are generally related by:
$<span class="math notranslate nohighlight">\(
H(X,Y)\leq H(X)+H(Y)
\)</span>$
with equality if and only if the variables are independent.</p>
</section>
<section id="conditional-entropy-h-x-y">
<h2>Conditional entropy, <span class="math notranslate nohighlight">\(H(X|Y)\)</span><a class="headerlink" href="#conditional-entropy-h-x-y" title="Link to this heading">#</a></h2>
<p>Quantifies how much uncertainty remains in <span class="math notranslate nohighlight">\(X\)</span> once the state of <span class="math notranslate nohighlight">\(Y\)</span> is known.</p>
<p>Formally:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H(X|Y)=\mathbb{E}_{X,Y}[h(X|Y)]=-\sum_{x\in \mathcal{X}\\ y\in \mathcal{Y}}P(x,y)\log P(x|y).
\end{split}\]</div>
<p>Note that we have broken away from the familiar “<span class="math notranslate nohighlight">\(p\log p\)</span>” formula here and the expectation is taken with respect to the joint distribution <span class="math notranslate nohighlight">\(P(x,y)\)</span>, since both variables contribute to the average uncertainty. Specifically:</p>
<ul class="simple">
<li><p>the <strong>log term</strong> uses the <strong>conditional probability</strong> <span class="math notranslate nohighlight">\(P(x|y)\)</span>, because we’re asking about the distribution of <span class="math notranslate nohighlight">\(X\)</span> given <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
<li><p>but the <strong>weights</strong> in the sum must still reflect how often each <span class="math notranslate nohighlight">\((x,y)\)</span> pair actually occurs and so the expectation is taken with respect to the <strong>joint distribution</strong> <span class="math notranslate nohighlight">\(P(x,y)\)</span>.</p></li>
<li><p>(if we only summed over <span class="math notranslate nohighlight">\(P(x|y)\)</span>, we’d be treating all values of <span class="math notranslate nohighlight">\(y\)</span> as equally likely. The joint distribution <span class="math notranslate nohighlight">\(P(x,y)\)</span> ensures the average is <strong>weighted correctly</strong> by how often each conditioning case occurs.)</p></li>
</ul>
<p>For a particular, fixed state <span class="math notranslate nohighlight">\(y\)</span>:</p>
<div class="math notranslate nohighlight">
\[
H(X|y) = \mathbb{E}_{X|y}[h(X|Y)] = -\sum_x P(x|y)\,\log P(x|y),
\]</div>
<p>which measures the uncertainty in <span class="math notranslate nohighlight">\(X\)</span> when <span class="math notranslate nohighlight">\(Y=y\)</span> is known.</p>
</section>
<section id="mutual-information-i-x-y">
<h2>Mutual information, <span class="math notranslate nohighlight">\(I(X; Y)\)</span><a class="headerlink" href="#mutual-information-i-x-y" title="Link to this heading">#</a></h2>
<p>A measure of <strong>statistical dependency</strong> between two random variables.</p>
<p>Answers: <em>how much does knowing <span class="math notranslate nohighlight">\(X\)</span> reduce uncertainty about <span class="math notranslate nohighlight">\(Y\)</span> (and vice versa)?</em></p>
<p>Formally:</p>
<div class="math notranslate nohighlight">
\[
I(X;Y) = \sum_{x,y} P(x,y)\,\log \frac{P(x,y)}{P(x)P(y)}.
\]</div>
<p>Key properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(I(X;Y) = 0\)</span>: <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent (knowing one tells you nothing about the other).</p></li>
<li><p><span class="math notranslate nohighlight">\(I(X;Y)\)</span> is always non-negative.</p></li>
<li><p>Maximum <span class="math notranslate nohighlight">\(I(X;Y)\)</span> occurs when one variable completely determines the other, in which case:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
I(X;Y) = H(X) = H(Y).
\]</div>
<p>Recall:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(x), P(y)\)</span> are the marginal probabilities of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> respectively</p></li>
<li><p><span class="math notranslate nohighlight">\(P(x,y)\)</span> is the <strong>joint probability distribution</strong>: the probability that variable <span class="math notranslate nohighlight">\(X\)</span> takes value <span class="math notranslate nohighlight">\(x\)</span> and variable <span class="math notranslate nohighlight">\(Y\)</span> takes value <span class="math notranslate nohighlight">\(y\)</span> simultaneously.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(x|y)\)</span> is the <strong>conditional probability</strong> of <span class="math notranslate nohighlight">\(X=x\)</span> given that <span class="math notranslate nohighlight">\(Y=y\)</span>.</p></li>
</ul>
</section>
<section id="many-others-too">
<h2>Many others too<a class="headerlink" href="#many-others-too" title="Link to this heading">#</a></h2>
<p>e.g. Relative entropy (Kullback-Liebler divergence)</p>
<p>…</p>
<p>We won’t get into the details here but you may come across them when researching for your Project and are welcome to use (or invent) any sensible information measure.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="putting-it-all-together">
<h1>Putting it all together<a class="headerlink" href="#putting-it-all-together" title="Link to this heading">#</a></h1>
<p><img alt="" src="../../_images/Entropy_VennDiagram.png" /></p>
<p>The area of each circle corresponds to the amount of uncertainty we have about the state of each variable (<em>not</em> the amount of information we have, which is a common misinterpretation).</p>
<p>(And these relationships generalise for more variables.)</p>
<p>Hence we can think about mutual information in terms of the aforementioned entropies:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
I(X; Y) &amp;= H(X) - H(X|Y) \\
&amp;= H(X) + H(Y) - H(X, Y) \\
&amp;= H(Y) - H(Y|X) \\
&amp;= H(X,Y) - H(Y|X) - H(X|Y)
\end{align*}
\end{split}\]</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="applications">
<h1>Applications<a class="headerlink" href="#applications" title="Link to this heading">#</a></h1>
<p>So far, our examples (like dice) assumed we knew the outcome probabilities in advance. In practice, we often do not — but <strong>information-theoretic quantities can still be computed directly from data</strong>.</p>
<p>Entropy has the general form:</p>
<div class="math notranslate nohighlight">
\[
H = -\sum p \log p,
\]</div>
<p>so once we can estimate the probabilities <span class="math notranslate nohighlight">\(p\)</span> from observations, we can calculate entropy and related measures.</p>
<section id="time-series-analysis">
<h2>Time series analysis<a class="headerlink" href="#time-series-analysis" title="Link to this heading">#</a></h2>
<p>Examples in time-series analysis include:</p>
<ul class="simple">
<li><p><strong>Permutation entropy</strong>: quantifies the diversity of ordinal patterns in a sequence.</p></li>
<li><p><strong>Transfer entropy</strong>: captures directional information flow between time series.</p></li>
</ul>
<p>Note that these extensions are not assessable in the Tests but are included as an example of how you might extend these ideas for your project.</p>
<section id="permutation-entropy">
<h3>Permutation Entropy<a class="headerlink" href="#permutation-entropy" title="Link to this heading">#</a></h3>
<p>Captures permutation patterns from a scalar time series measured from a complex system by organising windows of the time series with length <span class="math notranslate nohighlight">\(n\)</span>, according to their <em>relative values</em> forming the symbol <span class="math notranslate nohighlight">\(\pi_i\)</span>:.</p>
<p>Formally:</p>
<div class="math notranslate nohighlight">
\[
H(n) = -\sum_i^{n!} p(\pi_i)\log(p(\pi_i)).
\]</div>
<p>The simple example provided by Bandt and Pompe…</p>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(x=(4,7,9,10,6,11,3)\)</span> and <span class="math notranslate nohighlight">\(n=2\)</span></p>
<ul>
<li><p>we find find four pairs for which <span class="math notranslate nohighlight">\(x_{t+1}&gt;x_{t}\)</span> and two pairs for which <span class="math notranslate nohighlight">\(x_{t+1}&lt;x_{t}\)</span>, i.e.</p></li>
<li><p>4/6 pairs are represented by the permutation 01 (an increase)</p></li>
<li><p>2/6 are represented by 10 (a decrease)</p></li>
<li><p>permutation entropy (a measure of the probabilities of the permutations 01 and 10):</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[
H(2) = -\frac{2}{6}\log\left(\frac{2}{6}\right) - \frac{4}{6}\log\left(\frac{4}{6}\right) \approx 0.918
\]</div>
<!-- Bandt, Pompe, Phys Rev Lett. 2002 Apr 29;88(17):174102.  doi: 10.1103/PhysRevLett.88.174102 -->
<p>Everything you wanted to know about permutation entropy (and ordinal methods) is <a class="reference external" href="https://pubs.aip.org/collection/1214/Ordinal-Methods-Concepts-Applications-New?utm_%5B%E2%80%A6%5Ds%20and%20Challenges_Topical_2023&amp;amp;dm_i=1XPS,8DVWE,5T8ELS,YL4WO">here</a></p>
<p><img alt="" src="../../_images/PermutationEntropy.png" /></p>
<p>Image from Ordpy package intro <a class="reference external" href="https://doi.org/10.48550/arXiv.2102.06786">here</a>.</p>
</section>
<section id="transfer-entropy">
<h3>Transfer Entropy<a class="headerlink" href="#transfer-entropy" title="Link to this heading">#</a></h3>
<p>Based on conditional mutual information <span class="math notranslate nohighlight">\(I(X; Y |Z)\)</span> of the variables <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span> given the variable <span class="math notranslate nohighlight">\(Z\)</span>:</p>
<div class="math notranslate nohighlight">
\[
I(X; Y |Z) = H(X|Z) + H(Y |Z) - H(X, Y |Z)
\]</div>
<p>Schreiber introduced transfer entropy in 2000 as a means to detect asymmetries in interaction of two coupled dynamical systems from their time series.</p>
<p>It measures how much knowledge of the present state of <span class="math notranslate nohighlight">\(X\)</span> reduces uncertainty about the future of <span class="math notranslate nohighlight">\(Y\)</span>, beyond what is already explained by <span class="math notranslate nohighlight">\(Y\)</span>’s own past:</p>
<p>Formally:</p>
<div class="math notranslate nohighlight">
\[
T_{X\rightarrow Y} = I(X_t, Y_{t+1}|Y_t).
\]</div>
<p>In the context of animal interactions, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> will measure the behavior of two individuals and transfer entropy will be constructed based on raw time series extracted from behavioral observations or trajectory tracking:</p>
<p><img alt="" src="../../_images/TransferEntropy_Fish.png" /></p>
<p>Image from <a class="reference external" href="https://doi.org/10.1007/s11721-018-0157-x">here</a></p>
<p>Transfer entropy here provides insights into the phase transition. At the critical point, the transfer entropy exhibits unique behaviour, indicating a shift in the complexity of the system.</p>
<p>In the context of financial markets Korbel et al. analysed information flow between various market communities. They focussed on information transfer of rare events (typically large drops which can spread in the network):</p>
<p><img alt="" src="../../_images/TransferEntropy_StockExchange.png" /></p>
<p>Image from <a class="reference external" href="https://doi.org/10.3390/e21111124">here</a></p>
</section>
</section>
<section id="data-in-general">
<h2>Data in general<a class="headerlink" href="#data-in-general" title="Link to this heading">#</a></h2>
<p>Not all entropy-based extensions are tied to time series. Many are designed for static data, distributions or networks.</p>
<p>Fundamentally though, the process is the same:</p>
<ol class="arabic simple">
<li><p>Define/extract a probability distribution</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(-\sum p\log p\)</span> or a suitable variant of it</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks/week09"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../week08/L_Critical_phenomena.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Week 8: Critical behaviour</p>
      </div>
    </a>
    <a class="right-next"
       href="../week10/L_Game_theory.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Week 10: Game theory</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Week 9: Information Theory</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#what-exactly-is-complexity">WHAT exactly is ‘complexity’?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#where-is-complexity">WHERE is ‘complexity’?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-want-to-measure-complexity">WHY do we want to measure ‘complexity’?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-measure-complexity">HOW do we measure ‘complexity’?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#who-and-when-a-brief-origin-story">WHO and WHEN: a brief origin story</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#information">Information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy">Entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-as-expected-surprise">Entropy as expected surprise</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recall-expected-value">Recall: Expected value</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shannon-entropy-h-x">Shannon Entropy, <span class="math notranslate nohighlight">\(H(X)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing">Comparing</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summarising">Summarising</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#absolute-meaning">Absolute meaning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#relative-meaning">Relative meaning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-in-practice">Interpretation in practice</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-entropy-h-x-y">Joint entropy, <span class="math notranslate nohighlight">\(H(X,Y)\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-entropy-h-x-y">Conditional entropy, <span class="math notranslate nohighlight">\(H(X|Y)\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mutual-information-i-x-y">Mutual information, <span class="math notranslate nohighlight">\(I(X; Y)\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#many-others-too">Many others too</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting it all together</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#applications">Applications</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-series-analysis">Time series analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#permutation-entropy">Permutation Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-entropy">Transfer Entropy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-in-general">Data in general</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Shannon Algar
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>