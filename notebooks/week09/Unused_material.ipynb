{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471ad195-56c6-4e66-b341-567f972a0c60",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Spare/not dealt with material (2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2da91c-3a5b-4753-9101-ee882be6b934",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python\n",
    "## Load libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc0c95d-8120-416e-a5d6-b371bea427cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "Plundering done from here:\n",
    "https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/information-theory.ipynb#scrollTo=608e73cb \n",
    "\n",
    "We aim to make these ideas precise…. Information theory, entropy, maximum entropy methods, mutual information, transfer entropy. i.e. quantifying complexity. How information exists/is stored/transferred and what it tells us about the system.\n",
    "\n",
    "**Self-information:** The self-information is a measure of the amount of the information gained or 'surprise' associated with the occurrence of an event. The higher the self-information, the more informative the event/more surprising the fact the event occurred. It is calculated as $-\\log_2(p)$, where $p$ is the probability of the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb69741c-9938-43c7-bc45-ae3d12ac4666",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def self_information(p):\n",
    "    \"\"\"\n",
    "    Calculates the self-information of an event with probability p.\n",
    "\n",
    "    INPUTS:\n",
    "        p (float): The probability of the event.\n",
    "\n",
    "    OUTPUTS:\n",
    "        float: The self-information of the event.\n",
    "    \"\"\"\n",
    "    return -np.log2(p)\n",
    "\n",
    "#Example usage:\n",
    "self_information(1 / 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2d3df-925e-46a3-bc08-3a558dae0f05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Entropy**\n",
    "\n",
    "Entropy is a key concept in thermodynamics, statistical mechanics and information theory.\n",
    "\n",
    "It is a measure for the amount of 'disorder' and information in a system.\n",
    "\n",
    "In information theory, entropy describes how much randomness is present in a signal or a random event.\n",
    "\n",
    "Entropy of the degree distribution provides an average measurement of the heterogeneity of the network:\n",
    "\\begin{equation*}\n",
    "  H = -\\sum\\limits_{k} P(k) \\log P(k),\n",
    "\\end{equation*}\n",
    "where $P(k)$ is the degree distribution.\n",
    "\n",
    "Maximum value of entropy is obtained for a uniform degree distribution.\n",
    "\n",
    "Minimum value $H_{\\mathrm{min}}=0$ is achieved whenever all vertices have the same degree ($k$-regular network).\n",
    "\n",
    "**Entropy**\n",
    "\n",
    "Network entropy has been related to the robustness of networks, i.e.,their resilience to attacks, and in biological applications, the contribution of vertices to the network entropy is correlated with lethality in protein interactions networks.\n",
    "\n",
    "\n",
    "**Information entropy:** measures the amount of uncertainty/randomness/unpredictability in a set of data or a probability distribution. It quantifies the average amount of information required to describe the outcomes of a random variable. The higher the entropy the more uncertain we are about the outcomes.\n",
    "\n",
    "A fair die has a uniform distribution of outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d673c8b-98f8-4fd8-acfe-fba8ce46c007",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def information_entropy(p):\n",
    "    \"\"\"\n",
    "    Calculate the information entropy of a probability distribution.\n",
    "\n",
    "    INPUTS:\n",
    "        p (array-like): Input array representing a probability distribution.\n",
    "\n",
    "    OUTPUTS:\n",
    "        float: Entropy value of the probability distribution.\n",
    "\n",
    "    \"\"\"\n",
    "    information_entropy = - p * np.log2(p)\n",
    "    \n",
    "    out = np.nansum(information_entropy) # `nansum` sums up non-nan numbers, ignoring events with probability=0\n",
    "    return out\n",
    "\n",
    "fair_die_entropy = information_entropy(np.array([1/6, 1/6, 1/6, 1/6, 1/6, 1/6])) #a fair die\n",
    "loaded_die_entropy = information_entropy(np.array([0.5, 0.1, 0.1, 0.1, 0.1, 0.1])) #a loaded die\n",
    "\n",
    "print(fair_die_entropy, loaded_die_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b621a-8b5c-4376-9314-64905dbf39d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Thermodynamics\n",
    "Thermodynamics deals only with large systems, containing many constituents.\n",
    "\n",
    "phase space has a huge number of dimensions. \n",
    "\n",
    "The system may or may not be complex (irrelevant for the truth of the second law)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5493513-5dba-47e6-8e5a-ec027f6a19ed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Entropy\n",
    "entropy is simply a fancy word for “disorder”\n",
    "\n",
    "It is a quantitative measure\n",
    "\n",
    "The second law of thermodynamics is about order compared with disorder. One easy-to- grasp statement, which is an accurate one, is the following. It concerns the time-evolution of an isolated system, which means a system lacking any kind of interaction or connection with the rest of the universe. The spontaneous evolution of an isolated system can never lead to a decrease of its entropy (= disorder). The entropy is always increasing as long as the system evolves. If the system eventually reaches equilibrium and stops evolving, its entropy becomes constant. \n",
    "\n",
    "everybody knows the second law intuitively already. \n",
    "## 'Entropy'\n",
    "dimensionless entropy, which measures our lack of knowledge, is a purely subjective quantity.\n",
    "\n",
    "if entropy is really totally subjective, why does it appear so ob- jective to so many people? Ask any physical chemist. You will be told that entropy is a fundamental, permanent property of matter in bulk. You can measure it, you can calculate it accurately, you can look it up in tables, etc. And the answer to that new paradox is: large numbers. It is well-known that very large numbers have a way of making probabilities turn into absolute certainty.\n",
    "\n",
    "The fundamental building block of Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778536d-94ce-4145-a8fe-4c5c7e85a24e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### “the paradox of the arrow of time”\n",
    "it may stay constant, but this happens only under ideal conditions never realized in practice; in the real world, it is always increasing. Therefore the evolution of such a system is always irreversible: if you have observed a certain evolution of your system, you know that the backwards evolution, with the same things happening in reverse order of time, can never be possible, because it would make entropy decrease with time. And this comes as a big shock because, in mechanics, any possible motion is also possible in the reverse direction. The second law says that in thermodynamics this is never true: for an isolated system, no thermodynamic motion is ever reversible. It is the opposite of what happens in mechanics! How could this ever come about, since we are told that thermodynamics is just mechanics plus statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d9ecc-6b26-4ac6-83f7-de24dcb4bf12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def joint_entropy(p_xy):\n",
    "    joint_ent = -p_xy * np.log2(p_xy)\n",
    "    # Operator `nansum` will sum up the non-nan number\n",
    "    out = np.nansum(joint_ent)\n",
    "    return out\n",
    "\n",
    "joint_entropy(np.array([[0.1, 0.5], [0.1, 0.3]]))\n",
    "\n",
    "def conditional_entropy(p_xy, p_x):\n",
    "    p_y_given_x = p_xy/p_x\n",
    "    cond_ent = -p_xy * np.log2(p_y_given_x)\n",
    "    # Operator `nansum` will sum up the non-nan number\n",
    "    out = np.nansum(cond_ent)\n",
    "    return out\n",
    "\n",
    "conditional_entropy(np.array([[0.1, 0.5], [0.2, 0.3]]), np.array([0.2, 0.8]))\n",
    "\n",
    "def mutual_information(p_xy, p_x, p_y):\n",
    "    p = p_xy / (p_x * p_y)\n",
    "    mutual = p_xy * np.log2(p)\n",
    "    # Operator `nansum` will sum up the non-nan number\n",
    "    out = np.nansum(mutual)\n",
    "    return out\n",
    "\n",
    "mutual_information(np.array([[0.1, 0.5], [0.1, 0.3]]),\n",
    "                   np.array([0.2, 0.8]), np.array([[0.75, 0.25]]))\n",
    "\n",
    "def cross_entropy(y_hat, y):\n",
    "    ce = -np.log(y_hat[range(len(y_hat)), y])\n",
    "    return ce.mean()\n",
    "\n",
    "labels = np.array([0, 2])\n",
    "preds = np.array([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])\n",
    "\n",
    "cross_entropy(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed956935-9bb0-49d1-b531-bc60bc89b990",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Relative entropy\n",
    "\n",
    "The relative entropy measures the distance between two distributions and it is also called Kullback-Leibler distance. It is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297f78f-b4a6-499d-806f-120425184ffc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For 6 possible outcomes...\n",
    "# import numpy as np\n",
    "import math\n",
    "\n",
    "for i in range(1,6):\n",
    "    H = -(i/6)*math.log2(i/6)-(1-i/6)*math.log2(1-i/6)\n",
    "    print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f526b-f4d9-4401-99ab-904c5c2dd998",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "An example with maximally random situation to avoid confusion\n",
    "\n",
    "Connection of entropy to complexity\n",
    "\n",
    "Primary resource for this slide set: https://necsi.edu/chaos-complexity-and-entropy\n",
    "\n",
    "In Thermodynamics entropy is simply a fancy word for “disorder”\n",
    "\n",
    "You should consider the connection\n",
    "\n",
    "Include better description of bit cutting down space in half:\n",
    "    \n",
    "<center>\n",
    "<img src=\"3B1B_bits.png\" width=\"500\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7872c58-7890-4c31-94b1-2cbc1535b392",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate 1,000 random samples from a normal distribution with mean 0 and standard deviation 1\n",
    "data_normal = np.random.normal(0, 1, 1000)\n",
    "\n",
    "mean_normal = 0\n",
    "stddev_normal = 1\n",
    "x_normal = np.linspace(-3, 3, 1000)  # Range of values for PDF calculation\n",
    "pdf_normal = norm.pdf(x_normal, loc=mean_normal, scale=stddev_normal)\n",
    "entropy_normal = shannon_entropy(pdf_normal)\n",
    "\n",
    "# Generate random samples from a uniform distribution between 0 and 1\n",
    "data_uniform = np.random.uniform(0, 1, 1000)\n",
    "\n",
    "# Generate random samples from an exponential distribution with rate parameter (lambda) of 0.5\n",
    "data_exponential = np.random.exponential(1/0.5, 1000)\n",
    "\n",
    "# Generate random samples from a Poisson distribution with a mean rate of 3\n",
    "data_poisson = np.random.poisson(3, 1000)\n",
    "\n",
    "# Generate random samples from a binomial distribution with 10 trials and a success probability of 0.3\n",
    "data_binomial = np.random.binomial(10, 0.3, 1000)\n",
    "\n",
    "# Generate random samples from a geometric distribution with a success probability of 0.2\n",
    "data_geometric = np.random.geometric(0.2, 1000)\n",
    "\n",
    "# Create a histogram with 20 bins\n",
    "plt.hist(data_normal, bins=20, density=True, alpha=0.6, color='b')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf25519-3dd5-47af-aaa6-6d9af327b6c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure with 1 row and 3 columns of subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Define the range and number of outcomes\n",
    "lower_bound = 0\n",
    "upper_bound = 7\n",
    "num_outcomes = 1000\n",
    "\n",
    "# Generate random values from the uniform distribution\n",
    "samples_uniform = np.random.uniform(lower_bound, upper_bound, num_outcomes)\n",
    "\n",
    "# Create a histogram\n",
    "hist, bin_edges = np.histogram(samples_uniform, bins=20, range=(lower_bound, upper_bound))\n",
    "\n",
    "# Calculate the probability associated with each bin\n",
    "probabilities_uniform = hist / num_outcomes  # Normalize by the total number of outcomes\n",
    "\n",
    "# Calculate entropy using the estimated probabilities\n",
    "entropy_uniform = shannon_entropy(probabilities_uniform)\n",
    "axs[0].hist(samples_uniform, bins=20, density=True, color='powderblue')\n",
    "# axs[1].plot(x_uniform, pdf_normal, 'r-', lw=2)\n",
    "# axs[0].set_title(f'Uniform Distribution\\nEntropy: {entropy_uniform:.2f}')\n",
    "axs[0].set_xticks([])  # Remove x-axis ticks and labels\n",
    "axs[0].set_yticks([])  # Remove y-axis ticks and labels\n",
    "\n",
    "# Normal (Gaussian) Distribution\n",
    "mean_normal = 0\n",
    "stddev_normal = 1\n",
    "x_normal = np.linspace(-3, 3, 1000)\n",
    "# Create a histogram to estimate probabilities\n",
    "hist, bin_edges = np.histogram(samples_normal, bins=20, range=(-3, 3), density=True)\n",
    "\n",
    "# Calculate the probability associated with each bin\n",
    "probabilities_normal = hist * (bin_edges[1] - bin_edges[0])\n",
    "\n",
    "\n",
    "pdf_normal = norm.pdf(x_normal, loc=mean_normal, scale=stddev_normal)\n",
    "entropy_normal = shannon_entropy(probabilities_normal)\n",
    "axs[1].hist(samples_normal, bins=20, density=True, color='pink')\n",
    "# axs[1].plot(x_normal, pdf_normal, 'r-', lw=2)\n",
    "# axs[1].set_title(f'Normal Distribution\\nEntropy: {entropy_normal:.2f}')\n",
    "axs[1].set_xticks([])  # Remove x-axis ticks and labels\n",
    "axs[1].set_yticks([])  # Remove y-axis ticks and labels\n",
    "\n",
    "# Exponential Distribution\n",
    "rate_exponential = 0.1\n",
    "x_exponential = np.linspace(0, 10, 1000)\n",
    "samples_exponential = np.random.exponential(1/rate_exponential, 1000)\n",
    "\n",
    "pdf_exponential = expon.pdf(x_exponential, scale=1/rate_exponential)\n",
    "# Create a histogram to estimate probabilities\n",
    "hist, bin_edges = np.histogram(samples_exponential, bins=20, range=(0, 10), density=True)\n",
    "\n",
    "# Calculate the probability associated with each bin\n",
    "probabilities_exponential = hist * (bin_edges[1] - bin_edges[0])\n",
    "\n",
    "entropy_exponential = shannon_entropy(probabilities_exponential)\n",
    "axs[2].hist(samples_exponential, bins=20, density=True, color='navajowhite')\n",
    "# axs[2].plot(x_exponential, pdf_exponential, 'g-', lw=2)\n",
    "# axs[2].set_title(f'Exponential Distribution\\nEntropy: {entropy_exponential:.2f}')\n",
    "axs[2].set_xticks([])  # Remove x-axis ticks and labels\n",
    "axs[2].set_yticks([])  # Remove y-axis ticks and labels\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure to an image file (e.g., PNG, PDF, etc.)\n",
    "plt.savefig(\"entropy_figure.png\")  # Provide the desired file name and format\n",
    "\n",
    "# Display the figure\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f066a5b-4153-477e-976e-e459c8194bd6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shannon_entropy(probabilities):\n",
    "    entropy = 0\n",
    "    for p in probabilities:\n",
    "        if p > 0:\n",
    "            entropy -= p * np.log2(p)\n",
    "    return entropy\n",
    "    \n",
    "    # Define a function to calculate Shannon entropy\n",
    "def shannon_entropy(probabilities):\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "# Define the range and number of outcomes\n",
    "lower_bound = 0\n",
    "upper_bound = 7\n",
    "num_outcomes = 1000\n",
    "\n",
    "# Generate random values from the uniform distribution\n",
    "samples_uniform = np.random.uniform(lower_bound, upper_bound, num_outcomes)\n",
    "\n",
    "# Create a histogram\n",
    "hist, bin_edges = np.histogram(samples_uniform, bins=20, range=(lower_bound, upper_bound))\n",
    "\n",
    "# Calculate the probability associated with each bin\n",
    "probabilities_uniform = hist / num_outcomes  # Normalize by the total number of outcomes\n",
    "\n",
    "# Calculate entropy using the estimated probabilities\n",
    "entropy_uniform = shannon_entropy(probabilities_uniform)\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da702a7-1eb3-45d3-a3d8-fe0ee0b0d7f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For inclusion in 2024\n",
    "\"A new method of estimating the entropy and redundancy of a language is described.\" - Shannon's original paper looked at the redundancy of a language too\n",
    "https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf\n",
    "\n",
    "If the language is translated into binary digits(0 or 1) in the most efficient way, the entropy is the average number of binary digits required per letter of the original language. \n",
    "\n",
    "<!-- multi-scale: modelling entire structure is essentially impossible and the ability to think critically about the distinction between 'wholes' and 'parts' as well as understanding when it is possible to reduce the dimensionality with things like coarse-graining is key  -->\n",
    "\n",
    "\n",
    "Having defined $h(x)$...With the base of the log set to 2, each 'bit' of information you receive/surprise you experience corresponds to the total amount of possibilities being cut in half...\n",
    "\n",
    "... And this motivates a second way to view entropy.\n",
    "\n",
    "### B: Entropy as required information \n",
    "\n",
    "On average, what's the minimum number of yes/no questions required to determine the state of $X$?\n",
    "\n",
    "For the fair die: \n",
    "- Outcomes $1,2,5,6$: 3 questions each\n",
    "- Outcomes $3,4$: 2 questions each\n",
    "\n",
    "We can see this with a binary decision tree:\n",
    "\n",
    "![](images/BinaryDecisionTree_FairDie.png)\n",
    "\n",
    "The expected number of questions is:\n",
    "\n",
    "$$\n",
    "E[\\text{questions}] = \\left( \\frac{1}{6} \\times 3 \\right) + \\left( \\frac{1}{6} \\times 3 \\right) + \\left( \\frac{1}{6} \\times 2 \\right) + \\left( \\frac{1}{6} \\times 3 \\right) + \\left( \\frac{1}{6} \\times 3 \\right) + \\left( \\frac{1}{6} \\times 2 \\right) \\approx 2.67\n",
    "$$\n",
    "\n",
    "Entropy bound:\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_x \\tfrac{1}{6}\\log_2 \\tfrac{1}{6} \n",
    "= \\log_2 6 \\approx 2.59\n",
    "$$\n",
    "\n",
    "\n",
    "For the loaded die: \n",
    "- asking \"is it 2\", you'll be correct two times in three\n",
    "- if no ($1/3$ of the time): you still need to distinguish between the 5 remaining outcomes, each with probability $1/15$.\n",
    "    - this needs \\~$\\log_2 5 \\approx 2.32$ extra questions\n",
    "\n",
    "$$\n",
    "E[\\text{questions}] \n",
    "= \\tfrac{2}{3}(1) + \\tfrac{1}{3}(1+2.32) \n",
    "\\approx 1.78\n",
    "$$\n",
    "\n",
    "To minimise the number of questions, we might structure the decision tree as:\n",
    "\n",
    "![](images/BinaryDecisionTree_LoadedDie.png)\n",
    "\n",
    "The expected number of questions is then:\n",
    "\n",
    "$$\n",
    "E[\\text{questions}] = \\left(\\tfrac{2}{3} \\times 1 \\right) + \\left(\\tfrac{1}{3} \\times 3.32 \\right) \\approx 0.67 + 1.11 = 1.78\n",
    "$$\n",
    "\n",
    "Entropy bound:\n",
    "\n",
    "$$\n",
    "H(X) = -\\Big[\\tfrac{2}{3}\\log_2\\tfrac{2}{3} + 5\\cdot \\tfrac{1}{15}\\log_2\\tfrac{1}{15}\\Big] \n",
    "\\approx 1.69\n",
    "$$\n",
    "\n",
    "Efficient questioning strategies approach the entropy of the distribution. Entropy is the *ideal lower bound* (best possible average).\n",
    "In the case of the fair die, you need to ask more questions (that is to say, you need more information) to uniquely specify the state of the die, while in the case of the loaded one, since you already know that it's more likely to be two, you need less information\n",
    "\n",
    "The number of unique yes/no questions required is approximately given by the entropy function $H(X)$, when the logarithm base is 2.\n",
    "\n",
    "Note that inefficiencies in the questioning strategy will cause the average to be slightly greater than the entropy of the system.\n",
    "#### An alternative perspective...\n",
    "\n",
    "On average, what's the minimum number of yes/no questions required to determine the state of $X$?\n",
    "\n",
    "For the fair die: \n",
    "- on average, it will take you $\\approx2.59$ questions to uniquely determine the state of the die\n",
    "    - $I(x) = -\\log_2\\left(\\frac{1}{6}\\right) \\approx 2.59 \\text{ bits}$\n",
    "    - $H(X) = -6\\frac{1}{6}\\log_2\\left(\\frac{1}{6}\\right) \\approx 2.59 \\text{ bits}$\n",
    "\n",
    "For the loaded die with probability of rolling a 2 $p(2) = \\frac{2}{3}$\n",
    "- if you start with \"is it 2\", you'll be correct two times in three\n",
    "    - $I(2) = -\\log_2\\left(\\frac{2}{3}\\right) = \\log_2\\left(\\frac{3}{2}\\right) \\approx 0.585 \\text{ bits}$\n",
    "- Each of the outcomes 1, 3, 4, 5, 6 has a probability of $p(x) = \\frac{1}{15}$\n",
    "    - $I(x) = -\\log_2\\left(\\frac{1}{15}\\right) = \\log_2(15) \\approx 3.906 \\text{ bits}$\n",
    "- on average, it will only take $\\approx1.7$ guesses i.e. the **expected information content** (or Shannon entropy) is the weighted average of the information content over all possible outcomes:\n",
    "    - $H(X) = \\frac{2}{3} \\cdot 0.585 + \\frac{5}{15} \\cdot 3.906= 0.39 + 1.302 \\approx 1.692 \\text{ bits} $\n",
    "\n",
    "In the case of the fair die, you need to ask more questions (that is to say, you need more information) to uniquely specify the state of the die, while in the case of the loaded one, since you already know that it's more likely to be two, you need less information\n",
    "\n",
    "This (i.e. the number of unique yes/no questions required) is given by the entropy function $H(X)$, when the logarithm base is 2.\n",
    "\n",
    "<!-- This is linked with the idea of entropy as a measure of the “capacity” of a random variable to disclose information. You can’t really communicate that much information in a coin, since it can only be heads or tails. In contrast, you can communicate a huge amount of information in the English alphabet with its 26 characters (and correspondingly higher entropy). -->\n",
    "\n",
    "The entropy represents the best-case scenario, while the average number of questions reflects real-world questioning dynamics.\n",
    "\n",
    "When the probabilities are powers of $1/2$ (like a fair coin, or sometimes a fair 4- or 8-sided die), the binary tree is exact, and each outcome really does take the same number of questions.\n",
    "\n",
    "When they are not (like 5-sided or loaded dice), some outcomes take more questions, some fewer, and the average is what matters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (math3024)",
   "language": "python",
   "name": "math3024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
