{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c02aeed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Today we come full circle to understand one of the main questions that has run through everything we've covered since day one...\n",
    "\n",
    "**What exactly is 'complexity'?** \n",
    "\n",
    "We are trying to answer this because we don't have a precise definition of what a complex system is (new field finding its feet, multi-disciplinary with very large scope etc etc. All we have is a list of properties we require these systems to have, and even this isn't conclusive as they aren't all required all the time.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ff2c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- In the spirit of bringing everything full circle... -->\n",
    "# The scientific challenges of the 21st century thus far\n",
    "Climate change, pandemics, disinformation, economic inequality... \n",
    "\n",
    "A defining characteristic common to the many issues facing the modern world is that they deal with the problem of modelling, predicting, and controlling emergent properties of systems comprised of many interacting elements\n",
    "\n",
    "These systems exhibit complex phenomena such as synchronisation or critical phase changes, cascading failures, extreme ranges of intensities, self-organisation, ...\n",
    "\n",
    "<!-- multi-scale: modelling entire structure is essentially impossible and the ability to think critically about the distinction between 'wholes' and 'parts' as well as understanding when it is possible to reduce the dimensionality with things like coarse-graining is key  -->\n",
    "\n",
    "**Information theory** is a toolkit that can be applied to these systems and their emergent behaviour\n",
    "\n",
    ">'poised to become the lingua franca of our increasingly complexity-aware scientific enterprise' - *Information Theory for Complex Systems Scientists: What, Why, & How?*, Thomas F. Varley\n",
    "\n",
    "i.e. It's the bridge language that will let a climate scientist speak with a sociologist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a8d12f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Information Theory \n",
    "Developed by Claude Shannon when he was working at Bell Labs on the problem of how to reliably send a signal from a sender to a receiver over some potentially noisy channel (such as a telegraph line)\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"MidJ_EntropyMessage.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "Provides the mathematical foundation for understanding, measuring, and optimising the processes of communication, data storage, and processing\n",
    "\n",
    "One of the fundamental foundations on which our modern, digital world was built \"on par with the development of electricity, or the internal combustion engine in terms of impact\" \n",
    "<!-- (along with other transformative technologies like artificial intelligence (AI), blockchain, etc. I have no idea who actually said this) -->\n",
    "\n",
    "<!-- Almost entire theory was published in his *A Mathematical Theory of Communication* [1948] -->\n",
    "\n",
    "Whilst Information Theory has its origins in field of communication engineering it quickly generated interest from broad and interdisciplinary group of scientists, mathematicians, and even philosophers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2b2ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Information\n",
    "To Shannon, \"information\" referred to signals being propagated through a channel\n",
    "\n",
    "It deals with the general mathematics of *inference* (encoding, decoding, probability and statistics), particularly, inference under conditions of *uncertainty* \n",
    "\n",
    "Intuitively, it can be understood in terms of how our uncertainty is resolved by data made available to us (the problem of transmitting a message is a special case of this general framework)\n",
    "\n",
    "<!-- The same mathematics, applies to any issue where we are attempting to increase our degree of certainty about the state of some as-yet-unknown variable:  -->\n",
    "- information flow\n",
    "- the future state of an evolving system\n",
    "- the degree of mutual coupling between two variables\n",
    "- quantifying uncertainty or unpredictability\n",
    "- ...\n",
    "\n",
    "If these are the questions we are trying to address then of course this is a sensible framework for complex systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07457d15",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 'Information'\n",
    "A really important point is that 'information' is agnostic to the question of the \"meaning\" of a message\n",
    "\n",
    "i.e. We aren't interested in what the information represents - only that we have inferred the correct message from some space of possible messages\n",
    "\n",
    "The focus is on the quantity of information rather than its content or interpretation\n",
    "\n",
    "- **Information**: a measure of the reduction in uncertainty about a system or event when a signal is received\n",
    "- **Meaning**: involves interpretation and context, which are subjective and vary depending on the receiver. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31e5fdc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "e.g. Consider the message: *\"The sky is blue.\"*\n",
    "\n",
    "In the context of Information Theory, this sentence is just a sequence of characters\n",
    "\n",
    "We care about:\n",
    "- how many bits are needed to transmit the message?\n",
    "- what is the likelihood of errors in transmission?\n",
    "- how efficiently can the message be encoded and decoded?\n",
    "\n",
    "We don't care about:\n",
    "- what \"sky\" or \"blue\" mean\n",
    "- that this sentence conveys something about the weather\n",
    "\n",
    "These meanings arise in the mind of the receiver and involve interpretation, which is outside the scope of Information Theory\n",
    "\n",
    "Information Theory would treat both \"The sky is blue\" and some random characters \"ajksd873##@\" in much the same way: as strings of characters \n",
    "\n",
    "In fact, the random string is considered as having more information content because it is less predictable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801fafe5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Entropy \n",
    "Because information is the reduction of uncertainty associated with observing data, we must first understand **uncertainty**\n",
    "\n",
    "This is done with **\"entropy\"**, a mathematical function that quantifies how uncertain we are (on average) about the state of some variable *before* any message is received\n",
    "\n",
    "There are many definitions and ways to think about entropy - we will focus on two of the better ones\n",
    "<!-- Ignoring statistical entropy from physics/thermodynamics -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603c82d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Entropy as expected surprise\n",
    "Some events are more surprising than others... why?\n",
    "\n",
    "How surprised you are is directly related to how likely you were to make a correct prediction\n",
    "\n",
    "Consider a fair die\n",
    "- probability of any face is 1/6\n",
    "- if you were gambling on the die: no reason to pick any face over another \n",
    "- $\\therefore$ no outcome is more surprising than any other  \n",
    "\n",
    "This notion of surprise as being related to uncertainty becomes clearer if we instead imagine that you are a cheater and, at some point, manage to switch the fair die for a loaded one of your own\n",
    "\n",
    "Consider a loaded die \n",
    "- say 2 comes up 2/3 of the time, all other faces come up with probability 1/15\n",
    "- Knowing this probability distribution ahead of time, you will be less surprised to see a 2 come up than you would be to see a 2 come up\n",
    "\n",
    "So how do we quantify surprise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41a6ee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quantifying surprise, $h(x)$\n",
    "For a random variable $X$ that draws states from the support set $\\mathcal{X}$ according to the probability distribution $P(x)$, the surprise $h(x)$: \n",
    "1. associated with a particular outcome $X=x$ should be inversely proportional to the probability $P(x)$: the more probable an event, the less surprising it is\n",
    "2. should never be negative $h(x) \\leq 0$ for all $x \\in \\mathcal{X}$\n",
    "3. should be additive: each event has its own uncertainty, and when combined, we sum their respective amounts of information\n",
    "<!-- our surprise at the outcome of two simultaneous, independent coin flips, should be equal to our surprise at the outcome of the same coin flipped two different times. Formally: -->\n",
    "$$\\color{white}{h(x_1,x_2)=h(x_1)+h(x_2) ⇐⇒ X_1\\perp X_2}$$\n",
    "<!--  (because the joint probability of two independent events is the product of their individual probabilities) -->\n",
    "<!--  Multiplying surprise would imply some kind of compounding or interaction between the events -->\n",
    "4. should be continuous\n",
    "5. events that are guaranteed to happen are totally unsurprising: $P(x) = 1$ then $h(x) = 0$\n",
    "6. events that are impossible should be infinitely surprising: $P(x)=0$ then $h(x)=\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67663f5b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quantifying surprise, $h(x)$\n",
    "The function uniquely specified by these criteria is:\n",
    "$$h(x)=-\\log P(x)$$\n",
    "\n",
    "i.e. the more unlikely an outcome $x$ is, the greater the surprise when that outcome occurs\n",
    "- $P(x)$ is small (i.e., the event is unlikely), $h(x)$ will be large, indicating high surprise\n",
    "- $P(x)$ is large (i.e., the event is likely), $h(x)$ will be small, indicating low surprise\n",
    "\n",
    "$h(x)$ is referred to as the surprise, self-information, Shannon information content, or the local entropy of a specific outcome of random variable $X$\n",
    "\n",
    "With the base of the log set to 2, each 'bit' of information you receive/surprise you experience corresponds to the total amount of possibilities being cut in half...\n",
    "\n",
    "... And this motivates a second way to view entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f95568",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Entropy as required information \n",
    "\n",
    "On average, what's the minimum number of yes/no questions required to determine the state of $X$?\n",
    "\n",
    "For the fair die: \n",
    "- on average, it will take you $\\approx2.59$ questions to uniquely determine the state of the die\n",
    "\n",
    "We can see this with a binary decision tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa408c0d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"BinaryDecisionTree_FairDie.png\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "- For outcomes \\( 1, 2, 4, 5 \\), it takes 3 questions, and the probability of each outcome is $\\frac{1}{6}$\n",
    "- For outcomes \\( 3, 6 \\), it takes 2 questions, and the probability of each outcome is $\\frac{1}{6}$\n",
    "\n",
    "The expected number of questions is:\n",
    "$$\n",
    "E[\\text{questions}] = \\left( \\frac{1}{6} \\times 3 \\right) + \\left( \\frac{1}{6} \\times 3 \\right) + \\left( \\frac{1}{6} \\times 2 \\right) + \\left( \\frac{1}{6} \\times 3 \\right) + \\left( \\frac{1}{6} \\times 3 \\right) + \\left( \\frac{1}{6} \\times 2 \\right) \\approx 2.67\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a655e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "For the loaded die: \n",
    "- if you start with \"is it 2\", you'll be correct two times in three\n",
    "- on average, it will only take $\\approx1.7$ guesses\n",
    "\n",
    "To minimise the number of questions, we might structure the decision tree as:\n",
    "\n",
    "<center>\n",
    "<img src=\"BinaryDecisionTree_LoadedDie.png\" width=\"300\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae7340",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the case of the fair die, you need to ask more questions (that is to say, you need more information) to uniquely specify the state of the die, while in the case of the loaded one, since you already know that it's more likely to be two, you need less information\n",
    "\n",
    "The number of unique yes/no questions required is approximately given by the entropy function $H(X)$, when the logarithm base is 2\n",
    "\n",
    "(Note that inefficiencies in the questioning strategy will cause the average to be slightly greater than the entropy of the system.)\n",
    "<!-- Intuitively, the difference between the efficiency of the encoding and the entropy is a measure of the redundancy in the encoding). -->\n",
    "\n",
    "<!-- This is linked with the idea of entropy as a measure of the “capacity” of a random variable to disclose information. You can't really communicate that much information in a coin, since it can only be heads or tails. In contrast, you can communicate a huge amount of information in the English alphabet with its 26 characters (and correspondingly higher entropy). -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178649c4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Entropy as required information  (details)\n",
    "An alternative perspective...\n",
    "\n",
    "On average, what's the minimum number of yes/no questions required to determine the state of $X$?\n",
    "\n",
    "For the fair die: \n",
    "- on average, it will take you $\\approx2.59$ questions to uniquely determine the state of the die\n",
    "    - $I(x) = -\\log_2\\left(\\frac{1}{6}\\right) \\approx 2.59 \\text{ bits}$\n",
    "    - $H(X) = -6\\frac{1}{6}\\log_2\\left(\\frac{1}{6}\\right) \\approx 2.59 \\text{ bits}$\n",
    "\n",
    "For the loaded die with probability of rolling a 2 $p(2) = \\frac{2}{3}$\n",
    "- if you start with \"is it 2\", you'll be correct two times in three\n",
    "    - $I(2) = -\\log_2\\left(\\frac{2}{3}\\right) = \\log_2\\left(\\frac{3}{2}\\right) \\approx 0.585 \\text{ bits}$\n",
    "- Each of the outcomes 1, 3, 4, 5, 6 has a probability of $p(x) = \\frac{1}{15}$\n",
    "    - $I(x) = -\\log_2\\left(\\frac{1}{15}\\right) = \\log_2(15) \\approx 3.906 \\text{ bits}$\n",
    "- on average, it will only take $\\approx1.7$ guesses i.e. the **expected information content** (or Shannon entropy) is the weighted average of the information content over all possible outcomes:\n",
    "    - $H(X) = \\frac{2}{3} \\cdot 0.585 + \\frac{5}{15} \\cdot 3.906= 0.39 + 1.302 \\approx 1.692 \\text{ bits} $\n",
    "\n",
    "In the case of the fair die, you need to ask more questions (that is to say, you need more information) to uniquely specify the state of the die, while in the case of the loaded one, since you already know that it's more likely to be two, you need less information\n",
    "\n",
    "This (i.e. the number of unique yes/no questions required) is given by the entropy function $H(X)$, when the logarithm base is 2.\n",
    "\n",
    "<!-- This is linked with the idea of entropy as a measure of the “capacity” of a random variable to disclose information. You can’t really communicate that much information in a coin, since it can only be heads or tails. In contrast, you can communicate a huge amount of information in the English alphabet with its 26 characters (and correspondingly higher entropy). -->\n",
    "\n",
    "The entropy represents the best-case scenario, while the average number of questions reflects real-world questioning dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa5643",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Recall: Expected value\n",
    "The **expected value** of a random variable is computed by taking the sum of all possible outcomes, each weighted by its probability \n",
    "\n",
    "For a discrete random variable $X$, the expected value $\\mathbb{E}[X]$ is given by:\n",
    "\n",
    "$$\n",
    "\\color{white}{\\mathbb{E}[X] = \\sum_{x} p(x) \\cdot x}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $p(x)$ is the probability of outcome $x$\n",
    "- $x$ is the value of the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ad5e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shannon Entropy, $H(X)$\n",
    "\n",
    "Two important ideas are involved when we ask \"on average, how surprised are we as we watch this variable over time?\"\n",
    "- rare events are very surprising but we aren't that surprised very often\n",
    "- frequent events are unsurprising, so we spend a lot of time not being very surprised \n",
    "\n",
    "This can be quantified by the expected surprise of a distribution $P(X)$:\n",
    "$$\n",
    "H(X)=\\mathbb{E}_X[h(X)]=-\\sum_{x\\in \\mathcal{X}}P(x)\\log P(x)\n",
    "$$\n",
    "\n",
    "Note that by convention we say that $0 \\log(0) = 0$, so impossible events have no impact on our uncertainty\n",
    "\n",
    "Shannon entropy is *the* fundamental building block of Information Theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6897ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Consider some known distributions...\n",
    "\n",
    "Let $X$ be a normally distributed random variable, $X \\sim N(\\mu, \\sigma^2)$, 2ith probability density function (PDF) $f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(X) &= -\\int_{-\\infty}^{\\infty} f(x) \\log f(x) \\, dx \\\\\n",
    "\\therefore H_{\\text{normal}} &= -\\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) \\log \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\right) \\, dx \\\\\n",
    "&= -\\int_{-\\infty}^{\\infty} f(x) \\left(-\\frac{1}{2} \\log(2 \\pi \\sigma^2) - \\frac{(x - \\mu)^2}{2\\sigma^2}\\right) \\, dx\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The first term (the constant $\\frac{1}{2} \\log(2 \\pi \\sigma^2) $) comes out of the integral and integrates to 1, and the second term integrates to give $\\sigma^2 $. The result is:\n",
    "\n",
    "$$\n",
    "H_{\\text{normal}} = \\frac{1}{2} \\log(2 \\pi e \\sigma^2)\n",
    "$$\n",
    "\n",
    "Thus, the entropy of a normal distribution depends on the variance $\\sigma^2 $. The larger the variance, the more spread out the distribution, and thus the higher the entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dffabe6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $X$ be an exponentially distributed random variable, $X \\sim \\text{Exp}(\\lambda) $ with probability density function (PDF), $f(x) = \\lambda \\exp(-\\lambda x) \\quad \\text{for} \\, x \\geq 0$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(X) &= -\\int_{0}^{\\infty} f(x) \\log f(x) \\, dx \\\\\n",
    "\\therefore H_{\\text{exponential}} &= -\\int_{0}^{\\infty} \\lambda \\exp(-\\lambda x) \\log(\\lambda \\exp(-\\lambda x)) \\, dx \\\\\n",
    "&= -\\int_{0}^{\\infty} \\lambda \\exp(-\\lambda x) \\left(\\log(\\lambda) - \\lambda x\\right) \\, dx\\\\\n",
    "&= -\\log(\\lambda) \\int_{0}^{\\infty} \\lambda \\exp(-\\lambda x) \\, dx + \\lambda \\int_{0}^{\\infty} \\lambda x \\exp(-\\lambda x) \\, dx\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The first integral evaluates to 1, and the second integral evaluates to $\\frac{1}{\\lambda}$. Hence,\n",
    "\n",
    "$$\n",
    "H_{\\text{exponential}} = -\\log(\\lambda) + 1\n",
    "$$\n",
    "\n",
    "Thus, the entropy of an exponential distribution depends on the rate parameter $\\lambda$. The smaller the rate, the more spread out the distribution, and thus the higher the entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc3256",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparing\n",
    "The normal distribution's entropy grows with the variance $\\sigma^2$, while the exponential distribution's entropy is inversely related to the rate $\\lambda$\n",
    "\n",
    "For comparable scales (a similar spread/variance):\n",
    "\n",
    "Normal distribution $N(\\mu, \\sigma^2)$: \n",
    "- $H_{\\text{normal}} = \\frac{1}{2} \\log(2 \\pi e \\sigma^2)$\n",
    "- defined over the entire real line and spreads out more\n",
    "- generally has higher entropy\n",
    "\n",
    "Exponential distribution $\\text{Exp}(\\lambda)$: \n",
    "- (Recall that the variance of the exponential distribution is $\\frac{1}{\\lambda^2}$.)\n",
    "- $H_{\\text{exponential}} = 1 - \\log(\\lambda)$\n",
    "- despite its unbounded support, concentrates more heavily near 0 and decays rapidly\n",
    "- generally has lower entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae831452",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summarising\n",
    "\n",
    "Higher entropy indicates:\n",
    "- greater uncertainty/complexity/variability/predictability/...\n",
    "- richer interactions\n",
    "- more complex system \n",
    "\n",
    "Lower entropy indicates:\n",
    "- simpler\n",
    "- more ordered system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e7a63",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Joint entropy, $H(X,Y)$\n",
    "\n",
    "Measure of the total uncertainty in multiple random variables considered together\n",
    "\n",
    "Reflects both the individual uncertainties of the variables *and* how they interact with each other\n",
    "\n",
    "i.e. it provides insights into the interdependencies in a system (highly interconnected system typically has higher joint entropy)\n",
    "\n",
    "Defined as the union of both marginal entropies:\n",
    "\n",
    "$$\n",
    "\\color{white}{H(X,Y)=\\mathbb{E}_{X,Y}[h(X,Y)]=-\\sum_{x\\in \\mathcal{X}\\\\ y\\in \\mathcal{Y}}P(x,y)\\log P(x,y)}\n",
    "$$\n",
    "\n",
    "Joint and marginal entropies are generally related by:\n",
    "$$\n",
    "H(X,Y)\\leq H(X)+H(Y)\n",
    "$$\n",
    "with equality if and only if the variables are independent (rare)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf6c72",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Conditional entropy, $H(X|Y)$\n",
    "\n",
    "Quantifies how much uncertainty remains in $X$ after learning the state of $Y$\n",
    "\n",
    "<!-- i.e. it's the uncertainty specific to one variable that is left over after resolving the uncertainty specific to the other.  -->\n",
    "$$\n",
    "\\color{white}{H(X|Y)=\\mathbb{E}_{X,Y}[h(X|Y)]=-\\sum_{x\\in \\mathcal{X}\\\\ y\\in \\mathcal{Y}}P(x,y)\\log P(x|y)}\n",
    "$$\n",
    "\n",
    "Note that the expectation is taken with respect to the joint probability $P(X,Y)$ rather\n",
    "than the conditional probability $P(X|Y)$. The presence of two interacting variables means that their joint behaviour must be considered\n",
    "\n",
    "If we wanted to calculate the conditional entropy of $X$, given that we see $Y$ in some particular, fixed state $y$:\n",
    "\n",
    "$$\\color{white}{H(X|y)=\\mathbb{E}_{X|y}[h(X|Y)] = -\\sum P(x|y)\\log P(x|y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452b666",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mutual information, $I(X; Y)$\n",
    "\n",
    "The most general measure of correlations\n",
    "\n",
    "Mutual information (MI) provides a measure of the statistical dependency between two random variables: how much does knowing the state of $X$ reduce our uncertainty about the state of $Y$ (and vice versa)\n",
    "$$\n",
    "I(X;Y)=\\sum_{x,y} P(x,y)\\log\\frac{P(x,y)}{P(x)P(y)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $P(x), P(y)$ are the marginal probabilities of $X$ and $Y$ respectively\n",
    "\n",
    "\n",
    "$I(X;Y)=0$ implies that $X$ and $Y$ are independent, meaning knowing one provides no information about the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9313b8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Putting it all together\n",
    "\n",
    "The area of each circle corresponds to the amount of uncertainty we have about the state of each variable\n",
    "\n",
    "Note that it is *not* the amount of information we have (this is a common misinterpretation)\n",
    "<center>\n",
    "<img src=\"Entropy_VennDiagram.png\" width=\"400\"/>\n",
    "</center>\n",
    "\n",
    "(And these relationships generalise for more variables)\n",
    "\n",
    "\n",
    "Hence we can think about MI in terms of the aforementioned entropies:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(X; Y) &= H(X) - H(X|Y) \\\\\n",
    "&= H(X) + H(Y) - H(X, Y) \\\\\n",
    "&= H(Y) - H(Y|X) \\\\\n",
    "&= H(X,Y) - H(Y|X) - H(X|Y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<!-- i.e. it is the uncertainty that is common to both variables such that if we were to resolve all of our uncertainty about $Y$, we would also be resolving some uncertainty about $X$. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f48d6ea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Many others too\n",
    "e.g. Relative entropy (Kullback-Liebler divergence)\n",
    "\n",
    "...\n",
    "\n",
    "We won't get into the details here for a lack of time but you may come across them when researching for your Project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efb326",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applications\n",
    "\n",
    "We can compute the information-theoretic quantities without the prior knowledge of the outcome probabilities (like our die) - we can do it with data\n",
    "\n",
    "While the definition of these functionals is very general and elegant, the practical estimation faces challenges related to the problem of efficient estimation of the PDF from finite size samples\n",
    "\n",
    "Some examples of complexity measures where data is a time series..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4296894",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Permutation Entropy\n",
    "\n",
    "Captures permutation patterns from a scalar time series measured from a complex system by organising windows of the time series with length $n$, according to their *relative values* forming the symbol $\\pi_i$:  \n",
    "$$\n",
    "H(n) = -\\sum_i^{n!} p(\\pi_i)\\log(p(\\pi_i))\n",
    "$$\n",
    "\n",
    "The simple example provided by Bandt and Pompe...\n",
    "\n",
    "- For $x=(4,7,9,10,6,11,3)$ and $n=2$\n",
    "    - we find find four pairs for which $x_{t+1}>x_{t}$ and two pairs for which $x_{t+1}<x_{t}$, i.e.\n",
    "    - 4/6 pairs are represented by the permutation 01 (an increase)\n",
    "    - 2/6 are represented by 10 (a decrease)\n",
    "    - permutation entropy (a measure of the probabilities of the permutations 01 and 10):\n",
    "    $$\n",
    "    H(2) = -\\frac{2}{6}\\log\\left(\\frac{2}{6}\\right) - \\frac{4}{6}\\log\\left(\\frac{4}{6}\\right) \\approx 0.918\n",
    "    $$\n",
    "\n",
    "<!-- Bandt, Pompe, Phys Rev Lett. 2002 Apr 29;88(17):174102.  doi: 10.1103/PhysRevLett.88.174102 -->\n",
    "\n",
    "Everything you wanted to know about permutation entropy (and ordinal methods) but were afraid to ask: https://pubs.aip.org/collection/1214/Ordinal-Methods-Concepts-Applications-New?utm_[…]s%20and%20Challenges_Topical_2023&dm_i=1XPS,8DVWE,5T8ELS,YL4WO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e0803f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"PermutationEntropy.png\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "https://doi.org/10.48550/arXiv.2102.06786\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b18443",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transfer Entropy \n",
    "Note that **Transfer Entropy** is not assessable but is included as an example of how you might extend these ideas for your project\n",
    "\n",
    "The conditional mutual information $I(X; Y |Z)$ of the variables $X$, $Y$ given the variable $Z$:\n",
    "$$\n",
    "I(X; Y |Z) = H(X|Z) + H(Y |Z) - H(X, Y |Z)\n",
    "$$\n",
    "\n",
    "Schreiber introduced transfer entropy as a means to detect asymmetries in interaction of two coupled dynamical systems from their time series\n",
    "\n",
    "It is a measure of the reduction in the uncertainty of the future of $X$ from its present that is gained through knowledge of the present of $Y$, i.e. it is the conditional mutual information between $X_t$ and $Y_{t+1}$ conditional on $Y_t$:\n",
    "$$\n",
    "T_{X\\rightarrow Y} = I(X_t, Y_{t+1}|Y_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee13a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- In the context of animal interactions, $X$ and $Y$ will measure the behavior of two individuals and transfer entropy will be constructed based on raw time series extracted from behavioral observations or trajectory tracking\n",
    "<center>\n",
    "<img src=\"TransferEntropy_Fish.png\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "Entropy here provides insights into the phase transition. At the critical point, the entropy exhibits unique behaviour, indicating a shift in the complexity of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a058e5c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Korbel et al. analysed information flow between communities of financial markets, represented as complex networks (Entropy 2019, 21(11), 1124; https://doi.org/10.3390/e21111124). They focussed on information transfer of rare events—typically large drops which can spread in the network:\n",
    "<center>\n",
    "<img src=\"TransferEntropy_StockExchange.png\" width=\"600\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e517988c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae9bcb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a93158",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72233bfc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "Plundering done from here:\n",
    "https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/information-theory.ipynb#scrollTo=608e73cb \n",
    "\n",
    "We aim to make these ideas precise…. Information theory, entropy, maximum entropy methods, mutual information, transfer entropy. i.e. quantifying complexity. How information exists/is stored/transferred and what it tells us about the system.\n",
    "\n",
    "**Self-information:** The self-information is a measure of the amount of the information gained or 'surprise' associated with the occurrence of an event. The higher the self-information, the more informative the event/more surprising the fact the event occurred. It is calculated as $-\\log_2(p)$, where $p$ is the probability of the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800a6ddf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def self_information(p):\n",
    "    \"\"\"\n",
    "    Calculates the self-information of an event with probability p.\n",
    "\n",
    "    INPUTS:\n",
    "        p (float): The probability of the event.\n",
    "\n",
    "    OUTPUTS:\n",
    "        float: The self-information of the event.\n",
    "    \"\"\"\n",
    "    return -np.log2(p)\n",
    "\n",
    "#Example usage:\n",
    "self_information(1 / 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53feb54a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Entropy**\n",
    "\n",
    "Entropy is a key concept in thermodynamics, statistical mechanics and information theory.\n",
    "\n",
    "It is a measure for the amount of 'disorder' and information in a system.\n",
    "\n",
    "In information theory, entropy describes how much randomness is present in a signal or a random event.\n",
    "\n",
    "Entropy of the degree distribution provides an average measurement of the heterogeneity of the network:\n",
    "\\begin{equation*}\n",
    "  H = -\\sum\\limits_{k} P(k) \\log P(k),\n",
    "\\end{equation*}\n",
    "where $P(k)$ is the degree distribution.\n",
    "\n",
    "Maximum value of entropy is obtained for a uniform degree distribution.\n",
    "\n",
    "Minimum value $H_{\\mathrm{min}}=0$ is achieved whenever all vertices have the same degree ($k$-regular network).\n",
    "\n",
    "**Entropy**\n",
    "\n",
    "Network entropy has been related to the robustness of networks, i.e.,their resilience to attacks, and in biological applications, the contribution of vertices to the network entropy is correlated with lethality in protein interactions networks.\n",
    "\n",
    "\n",
    "**Information entropy:** measures the amount of uncertainty/randomness/unpredictability in a set of data or a probability distribution. It quantifies the average amount of information required to describe the outcomes of a random variable. The higher the entropy the more uncertain we are about the outcomes.\n",
    "\n",
    "A fair die has a uniform distribution of outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "670e06e7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.584962500721156 2.1609640474436813\n"
     ]
    }
   ],
   "source": [
    "def information_entropy(p):\n",
    "    \"\"\"\n",
    "    Calculate the information entropy of a probability distribution.\n",
    "\n",
    "    INPUTS:\n",
    "        p (array-like): Input array representing a probability distribution.\n",
    "\n",
    "    OUTPUTS:\n",
    "        float: Entropy value of the probability distribution.\n",
    "\n",
    "    \"\"\"\n",
    "    information_entropy = - p * np.log2(p)\n",
    "    \n",
    "    out = np.nansum(information_entropy) # `nansum` sums up non-nan numbers, ignoring events with probability=0\n",
    "    return out\n",
    "\n",
    "fair_die_entropy = information_entropy(np.array([1/6, 1/6, 1/6, 1/6, 1/6, 1/6])) #a fair die\n",
    "loaded_die_entropy = information_entropy(np.array([0.5, 0.1, 0.1, 0.1, 0.1, 0.1])) #a loaded die\n",
    "\n",
    "print(fair_die_entropy, loaded_die_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76dbc8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Thermodynamics\n",
    "Thermodynamics deals only with large systems, containing many constituents.\n",
    "\n",
    "phase space has a huge number of dimensions. \n",
    "\n",
    "The system may or may not be complex (irrelevant for the truth of the second law)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6d1ef",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Entropy\n",
    "entropy is simply a fancy word for “disorder”\n",
    "\n",
    "It is a quantitative measure\n",
    "\n",
    "The second law of thermodynamics is about order compared with disorder. One easy-to- grasp statement, which is an accurate one, is the following. It concerns the time-evolution of an isolated system, which means a system lacking any kind of interaction or connection with the rest of the universe. The spontaneous evolution of an isolated system can never lead to a decrease of its entropy (= disorder). The entropy is always increasing as long as the system evolves. If the system eventually reaches equilibrium and stops evolving, its entropy becomes constant. \n",
    "\n",
    "everybody knows the second law intuitively already. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df499587",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Joint entropy:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08952076",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 'Entropy'\n",
    "dimensionless entropy, which measures our lack of knowledge, is a purely subjective quantity.\n",
    "\n",
    "if entropy is really totally subjective, why does it appear so ob- jective to so many people? Ask any physical chemist. You will be told that entropy is a fundamental, permanent property of matter in bulk. You can measure it, you can calculate it accurately, you can look it up in tables, etc. And the answer to that new paradox is: large numbers. It is well-known that very large numbers have a way of making probabilities turn into absolute certainty.\n",
    "\n",
    "The fundamental building block of Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc6bff",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### “the paradox of the arrow of time”\n",
    "it may stay constant, but this happens only under ideal conditions never realized in practice; in the real world, it is always increasing. Therefore the evolution of such a system is always irreversible: if you have observed a certain evolution of your system, you know that the backwards evolution, with the same things happening in reverse order of time, can never be possible, because it would make entropy decrease with time. And this comes as a big shock because, in mechanics, any possible motion is also possible in the reverse direction. The second law says that in thermodynamics this is never true: for an isolated system, no thermodynamic motion is ever reversible. It is the opposite of what happens in mechanics! How could this ever come about, since we are told that thermodynamics is just mechanics plus statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cc24b59",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6854752972273346"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def joint_entropy(p_xy):\n",
    "    joint_ent = -p_xy * np.log2(p_xy)\n",
    "    # Operator `nansum` will sum up the non-nan number\n",
    "    out = np.nansum(joint_ent)\n",
    "    return out\n",
    "\n",
    "joint_entropy(np.array([[0.1, 0.5], [0.1, 0.3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcb53989",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8635472023399721"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conditional_entropy(p_xy, p_x):\n",
    "    p_y_given_x = p_xy/p_x\n",
    "    cond_ent = -p_xy * np.log2(p_y_given_x)\n",
    "    # Operator `nansum` will sum up the non-nan number\n",
    "    out = np.nansum(cond_ent)\n",
    "    return out\n",
    "\n",
    "conditional_entropy(np.array([[0.1, 0.5], [0.2, 0.3]]), np.array([0.2, 0.8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "457bce3e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7194602975157967"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mutual_information(p_xy, p_x, p_y):\n",
    "    p = p_xy / (p_x * p_y)\n",
    "    mutual = p_xy * np.log2(p)\n",
    "    # Operator `nansum` will sum up the non-nan number\n",
    "    out = np.nansum(mutual)\n",
    "    return out\n",
    "\n",
    "mutual_information(np.array([[0.1, 0.5], [0.1, 0.3]]),\n",
    "                   np.array([0.2, 0.8]), np.array([[0.75, 0.25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24c0de8e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9485599924429406"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    ce = -np.log(y_hat[range(len(y_hat)), y])\n",
    "    return ce.mean()\n",
    "\n",
    "labels = np.array([0, 2])\n",
    "preds = np.array([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])\n",
    "\n",
    "cross_entropy(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e51bf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Relative entropy\n",
    "\n",
    "The relative entropy measures the distance between two distributions and it is also called Kullback-Leibler distance. It is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "039ed188",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6500224216483541\n",
      "0.9182958340544896\n",
      "1.0\n",
      "0.9182958340544896\n",
      "0.6500224216483541\n"
     ]
    }
   ],
   "source": [
    "# For 6 possible outcomes...\n",
    "# import numpy as np\n",
    "import math\n",
    "\n",
    "for i in range(1,6):\n",
    "    H = -(i/6)*math.log2(i/6)-(1-i/6)*math.log2(1-i/6)\n",
    "    print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a0187",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# How do previous week's topics relate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c086c42",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Previous topics:**\n",
    "\n",
    "*Complex Systems (Schelling)* Computation of maximum entropy: https://github.com/cosmoharrigan/matrix-entropy. Blog explanation: https://stats.stackexchange.com/questions/17109/measuring-entropy-information-patterns-of-a-2d-binary-matrix\n",
    "\n",
    "*Reaction-diffusion*\n",
    "\n",
    "*Cellular automata* \n",
    "CA as models of complexity: https://www.nature.com/articles/311419a0.pdf\n",
    "We can compute the entropy of a cellular automata. For example, consider an elementary CA. A given rule will have a certain probability of black/white cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4ad52",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Bounds\n",
    "\n",
    "The concavity of log\n",
    "log\n",
    " and Jensen's inequality is enough to show the upper bound :\n",
    "\n",
    "𝐻(𝑋)=𝔼(log21ℙ𝑋(𝑋))≤log2𝔼(1ℙ𝑋(𝑋))(Jensen's inequality)=log2||\n",
    "\n",
    "https://math.stackexchange.com/questions/1114589/entropy-is-upper-bounded-by-cardinality-of-the-random-variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c248665",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "An example with maximally random situation to avoid confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb324c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "Connection of entropy to complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2b03c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Primary resource for this slide set: https://necsi.edu/chaos-complexity-and-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28967f0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "In Thermodynamics entropy is simply a fancy word for “disorder”\n",
    "\n",
    "You should consider the connection\n",
    "\n",
    "Include better description of bit cutting down space in half:\n",
    "    \n",
    "<center>\n",
    "<img src=\"3B1B_bits.png\" width=\"500\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3acd8b6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11547a810>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate 1,000 random samples from a normal distribution with mean 0 and standard deviation 1\n",
    "data_normal = np.random.normal(0, 1, 1000)\n",
    "\n",
    "mean_normal = 0\n",
    "stddev_normal = 1\n",
    "x_normal = np.linspace(-3, 3, 1000)  # Range of values for PDF calculation\n",
    "pdf_normal = norm.pdf(x_normal, loc=mean_normal, scale=stddev_normal)\n",
    "entropy_normal = shannon_entropy(pdf_normal)\n",
    "\n",
    "# Generate random samples from a uniform distribution between 0 and 1\n",
    "data_uniform = np.random.uniform(0, 1, 1000)\n",
    "\n",
    "# Generate random samples from an exponential distribution with rate parameter (lambda) of 0.5\n",
    "data_exponential = np.random.exponential(1/0.5, 1000)\n",
    "\n",
    "# Generate random samples from a Poisson distribution with a mean rate of 3\n",
    "data_poisson = np.random.poisson(3, 1000)\n",
    "\n",
    "# Generate random samples from a binomial distribution with 10 trials and a success probability of 0.3\n",
    "data_binomial = np.random.binomial(10, 0.3, 1000)\n",
    "\n",
    "# Generate random samples from a geometric distribution with a success probability of 0.2\n",
    "data_geometric = np.random.geometric(0.2, 1000)\n",
    "\n",
    "# Create a histogram with 20 bins\n",
    "plt.hist(data_normal, bins=20, density=True, alpha=0.6, color='b')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1129309d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Create a figure with 1 row and 3 columns of subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Define the range and number of outcomes\n",
    "lower_bound = 0\n",
    "upper_bound = 7\n",
    "num_outcomes = 1000\n",
    "\n",
    "# Generate random values from the uniform distribution\n",
    "samples_uniform = np.random.uniform(lower_bound, upper_bound, num_outcomes)\n",
    "\n",
    "# Create a histogram\n",
    "hist, bin_edges = np.histogram(samples_uniform, bins=20, range=(lower_bound, upper_bound))\n",
    "\n",
    "# Calculate the probability associated with each bin\n",
    "probabilities_uniform = hist / num_outcomes  # Normalize by the total number of outcomes\n",
    "\n",
    "# Calculate entropy using the estimated probabilities\n",
    "entropy_uniform = shannon_entropy(probabilities_uniform)\n",
    "axs[0].hist(samples_uniform, bins=20, density=True, color='powderblue')\n",
    "# axs[1].plot(x_uniform, pdf_normal, 'r-', lw=2)\n",
    "# axs[0].set_title(f'Uniform Distribution\\nEntropy: {entropy_uniform:.2f}')\n",
    "axs[0].set_xticks([])  # Remove x-axis ticks and labels\n",
    "axs[0].set_yticks([])  # Remove y-axis ticks and labels\n",
    "\n",
    "# Normal (Gaussian) Distribution\n",
    "mean_normal = 0\n",
    "stddev_normal = 1\n",
    "x_normal = np.linspace(-3, 3, 1000)\n",
    "# Create a histogram to estimate probabilities\n",
    "hist, bin_edges = np.histogram(samples_normal, bins=20, range=(-3, 3), density=True)\n",
    "\n",
    "# Calculate the probability associated with each bin\n",
    "probabilities_normal = hist * (bin_edges[1] - bin_edges[0])\n",
    "\n",
    "\n",
    "pdf_normal = norm.pdf(x_normal, loc=mean_normal, scale=stddev_normal)\n",
    "entropy_normal = shannon_entropy(probabilities_normal)\n",
    "axs[1].hist(samples_normal, bins=20, density=True, color='pink')\n",
    "# axs[1].plot(x_normal, pdf_normal, 'r-', lw=2)\n",
    "# axs[1].set_title(f'Normal Distribution\\nEntropy: {entropy_normal:.2f}')\n",
    "axs[1].set_xticks([])  # Remove x-axis ticks and labels\n",
    "axs[1].set_yticks([])  # Remove y-axis ticks and labels\n",
    "\n",
    "# Exponential Distribution\n",
    "rate_exponential = 0.1\n",
    "x_exponential = np.linspace(0, 10, 1000)\n",
    "samples_exponential = np.random.exponential(1/rate_exponential, 1000)\n",
    "\n",
    "pdf_exponential = expon.pdf(x_exponential, scale=1/rate_exponential)\n",
    "# Create a histogram to estimate probabilities\n",
    "hist, bin_edges = np.histogram(samples_exponential, bins=20, range=(0, 10), density=True)\n",
    "\n",
    "# Calculate the probability associated with each bin\n",
    "probabilities_exponential = hist * (bin_edges[1] - bin_edges[0])\n",
    "\n",
    "entropy_exponential = shannon_entropy(probabilities_exponential)\n",
    "axs[2].hist(samples_exponential, bins=20, density=True, color='navajowhite')\n",
    "# axs[2].plot(x_exponential, pdf_exponential, 'g-', lw=2)\n",
    "# axs[2].set_title(f'Exponential Distribution\\nEntropy: {entropy_exponential:.2f}')\n",
    "axs[2].set_xticks([])  # Remove x-axis ticks and labels\n",
    "axs[2].set_yticks([])  # Remove y-axis ticks and labels\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure to an image file (e.g., PNG, PDF, etc.)\n",
    "plt.savefig(\"entropy_figure.png\")  # Provide the desired file name and format\n",
    "\n",
    "# Display the figure\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bcd4e895",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def shannon_entropy(probabilities):\n",
    "    entropy = 0\n",
    "    for p in probabilities:\n",
    "        if p > 0:\n",
    "            entropy -= p * np.log2(p)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b607c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to calculate Shannon entropy\n",
    "def shannon_entropy(probabilities):\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e2f962d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45, 49, 48, 56, 43, 53, 50, 50, 43, 52, 54, 53, 53, 48, 64, 55, 41,\n",
       "       48, 43, 52])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the range and number of outcomes\n",
    "lower_bound = 0\n",
    "upper_bound = 7\n",
    "num_outcomes = 1000\n",
    "\n",
    "# Generate random values from the uniform distribution\n",
    "samples_uniform = np.random.uniform(lower_bound, upper_bound, num_outcomes)\n",
    "\n",
    "# Create a histogram\n",
    "hist, bin_edges = np.histogram(samples_uniform, bins=20, range=(lower_bound, upper_bound))\n",
    "\n",
    "# Calculate the probability associated with each bin\n",
    "probabilities_uniform = hist / num_outcomes  # Normalize by the total number of outcomes\n",
    "\n",
    "# Calculate entropy using the estimated probabilities\n",
    "entropy_uniform = shannon_entropy(probabilities_uniform)\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b18c9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# For inclusion in 2024\n",
    "\"A new method of estimating the entropy and redundancy of a language is described.\" - Shannon's original paper looked at the redundancy of a language too\n",
    "https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf\n",
    "\n",
    "If the language is translated into binary digits(0 or 1) in the most efficient way, the entropy is the average number of binary digits required per letter of the original language. "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
