{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c576cfe-6ea6-4b81-bf21-5d65d2dc0b5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "# Week 9: Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b8784d-0010-4691-acfb-9d99cceb4214",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "![](images/ChatGPT_Complexity.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e60eee-d8cc-4bd5-a080-dd5326216045",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "We will now come full circle to try understand one of the main questions that has run through everything we've covered since day one..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c287fdd7-3023-4a73-9b7b-5ea0835bf95d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "# WHAT exactly is 'complexity'?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed4ba0-0f23-470c-880b-f3eb1daa8cce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "We are trying to answer this because we don't have a precise definition of what a complex system is (new field finding its feet, multi-disciplinary with very large scope etc etc). All we really have is a list of properties that we require these systems to have (and even this isn't conclusive as they aren't all required all the time).\n",
    "\n",
    "Before we tackle this question let's answer some easier ones..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce31754-cfca-47a9-8dc9-b262830f8113",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "# WHERE is 'complexity'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b88d0cd-1a46-454a-9a96-e158bcc32fb5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "By now, I hope that it is clear that **complex systems are everywhere**.\n",
    "\n",
    "They appear in the natural world:\n",
    "- ecosystems, food webs, and climate,\n",
    "- the brain, immune system, and genetic regulation,\n",
    "- fractals and turbulence in physical systems.\n",
    "\n",
    "They appear in human society:\n",
    "- economies and financial markets,\n",
    "- cities, transport, and communication networks,\n",
    "- cultural diffusion, cooperation, and conflict.\n",
    "\n",
    "They appear in technology:\n",
    "- the internet, power grids, and social media,\n",
    "- artificial intelligence and multi-agent systems.\n",
    "\n",
    "Complex systems are unpredictable. Wherever there are many interacting parts giving rise to emergent patterns, we find complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f86d71-a8ec-490f-b78b-51cd29a1ccf3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "# WHY do we want to measure 'complexity'?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9883c-4508-4933-8faf-7bfbfa25ff81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "The defining scientific challenges of the 21st century — climate change, pandemics, disinformation, economic inequality — all share something in common. They are **emergent problems** of complex systems: unpredictable outcomes from countless interactions among individuals, institutions, and technologies.\n",
    "\n",
    "Complex systems often display behaviours like synchronisation, critical phase changes, cascading failures, heavy-tailed events, and self-organisation. We already have tools to model such behaviour and now seek to better quantify it.\n",
    "\n",
    "By measuring complexity we can move beyond description to characterise and compare systems:\n",
    "\n",
    "- **compare systems or models**: judge whether one is “more complex” than another; useful for testing alternatives or contrasting natural vs. artificial systems.\n",
    "- **track behaviour and transitions**: complexity changes over time and signals shifts from order to chaos or tipping points.\n",
    "    - ecosystems: rising complexity may signal diversification; collapse shows loss of complexity.\n",
    "    - physics/ABMs: changes in correlation length or entropy may reveal criticality.\n",
    "- **evaluate model fidelity**: does the model capture the richness of real dynamics, not just averages?\n",
    "- **quantify information content**: how much information is encoded in structure or dynamics (this week’s focus).\n",
    "- **aid prediction and control**: between simplicity and randomness lies the “edge of chaos” where adaptation and learning are possible.\n",
    "- **communicate across disciplines**: complexity measures offer a *common language* across biology, physics, sociology, economics, and beyond.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31e1dc-c201-432f-9478-59caf06a1a08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "<div style=\"margin: 1.5em 0; border-right: 4px solid #888; padding-right: 1em; text-align: right;\">\n",
    "  <div style=\"font-style: italic; font-family: 'Comic Neue', 'Segoe Script', cursive; font-size: 1.2em;\">\n",
    "    [Information theory is] poised to become the lingua franca of our increasingly complexity-aware scientific enterprise  </div>\n",
    "  <div style=\"font-style: normal; font-size: 0.95em; margin-top: 0.5em;\">\n",
    "    — Information Theory for Complex Systems Scientists: What, Why, & How?, Thomas F. Varley\n",
    "\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632fdfe2-ea58-478d-b098-e94d18a20aff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "# HOW do we measure 'complexity'?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9720d7a-2686-4352-be14-0e51ce69610b",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "How do we give a climate scientist and a sociologist, or a neuroscientist and an economist, a *common language* so that they can compare complexity across disciplines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e75d3a-e03e-412c-aeb9-4a5e1dd0b0f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "We have already seen one measure of complexity... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233fd53-9464-4e38-8fdf-a3896cf71de8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "**Fractal dimension**. \n",
    "\n",
    "A larger fractal dimension means the object fills space more densely, capturing self-similarity across scales. e.g.:\n",
    "- the Koch curve has fractal dimension $\\approx 1.26$: more complex than a straight line (dimension 1) but far from filling a plane. \n",
    "- the Peano curve has fractal dimension 2, fully covering the plane.\n",
    "\n",
    "We have also used order parameters (e.g. magnetisation/alignment in the Ising/Vicsek model, synchronisation in Kuramoto oscillators). While not always labelled as complexity measures, they serve a similar purpose: reducing high-dimensional behaviour to a single number that tracks emergent organisation.\n",
    "\n",
    "These measures are valuable but limited. We need something that can capture not just structure or organisation, but also the unpredictability and variability of system behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efb75c6-d368-4cf1-ade8-114fa225fef0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "# WHO and WHEN: a brief origin story\n",
    "\n",
    "Information theory was born in 1948, when Claude Shannon, working at Bell Labs, tackled a practical engineering problem: *How can a message be transmitted reliably over a noisy channel?*\n",
    "\n",
    "![](images/Claude_Shannon.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079084c3-a0c2-401c-ae60-47195e818053",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "From this question, Shannon developed the mathematics of information: how to quantify, encode, and transmit it efficiently. His framework laid the foundation for digital communication, computing, and data science, and has been described as *“on par with electricity or the internal combustion engine in terms of impact.”*\n",
    "\n",
    "Although Shannon’s immediate focus was telegraphs and phone lines, his theory of measuring uncertainty, structure, and order proved far more general. It has since influenced physics, biology, mathematics, computer science, and philosophy.\n",
    "\n",
    "Today, information theory underpins applications such as:\n",
    "- how your phone compresses photos,\n",
    "- quantifying genetic information in DNA,\n",
    "- measuring correlations in neural networks,\n",
    "- detecting critical transitions in ecosystems,\n",
    "- modelling the spread of ideas and disinformation in society."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7292d1a-7dd3-46e1-94bd-4e5b96ef23a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "presenter-notes",
     "remove-cell"
    ]
   },
   "source": [
    "<div style=\"background-color: #ffff88; padding: 1em; border-left: 5px solid #fff102;\">\n",
    "\n",
    "Regarding the above quote: along with other transformative technologies like artificial intelligence (AI), blockchain, etc. I have no idea who actually said this.\n",
    "\n",
    "Almost entire theory was published in his *A Mathematical Theory of Communication* [1948]\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997874c3-83f4-48aa-bd97-8c6cc29edbfb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "Information theory is a toolkit to quantify complexity itself.\n",
    "\n",
    "We will look extensively at entropy as the key measure.\n",
    "\n",
    "1. **Entropy**: measures uncertainty and disorder in a system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f842d-584b-44d0-a26a-47e37c9cf13b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "\n",
    "And then consider some variants:\n",
    "\n",
    "2. **Joint entropy**: quantifies the total uncertainty of two (or more) variables taken together.\n",
    "3. **Conditional entropy**: measures the remaining uncertainty in one variable given knowledge of another.\n",
    "4. **Mutual information**: reveals hidden dependencies, capturing how much knowing one variable reduces uncertainty about another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a37f33-88ff-478d-9f0b-0f354b979c79",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "These quantities are universal. They apply whether we’re analysing spins in the Ising model, birds in a flock, or rumours on social media."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ee94b-1632-413c-b1c6-dcaace21c5fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "To summarise the roadmap for this topic:\n",
    "\n",
    "We need a way to *quantify uncertainty* in outcomes of complex systems.\n",
    "- information reduces it,\n",
    "- entropy quantifies it,\n",
    "- and the variants let us capture relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2b2ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "## Information\n",
    "\n",
    "Information is what reduces our uncertainty when data is revealed.\n",
    "\n",
    "To Shannon, \"information\" referred to signals transmitted through a channel.\n",
    "\n",
    "Intuitively, information measures *how much our uncertainty is reduced when new data becomes available*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7412b-c838-4651-a864-f7b4277ed4ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "The problem of transmitting a message is just one special case of this general framework. The same mathematics applies whenever we seek to reduce uncertainty about an unknown variable, such as:\n",
    "- tracking information flow,\n",
    "- predicting the future state of a system,\n",
    "- measuring coupling between variables,\n",
    "- quantifying unpredictability.\n",
    "\n",
    "This is why information theory provides a natural framework for studying complex systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07457d15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "A really important point is that 'information' is agnostic to the question of the \"meaning\" of a message.\n",
    "\n",
    "- **Information**: a measure of the reduction in uncertainty about a system or event when a signal is received.\n",
    "- **Meaning**: involves interpretation and context, which are subjective and vary depending on the receiver. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4853c503-0f04-4e8a-ad76-ca657449f386",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "i.e. We aren't interested in what the information represents - only that we have inferred the correct message from some space of possible messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31e5fdc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "e.g. Consider the message: \n",
    "\n",
    "<div style=\"text-align: center; color: blue; font-family: 'Georgia', serif; font-size: 22px;\">\n",
    "<em>\"The sky is blue.\"</em>\n",
    "</div>\n",
    "\n",
    "In the context of Information Theory, this sentence is just a sequence of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e53c64-7ca1-4adb-bdd7-65b3be92587d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "We care about:\n",
    "- how many bits are needed to transmit the message?\n",
    "- what is the likelihood of errors in transmission?\n",
    "- how efficiently can the message be encoded and decoded?\n",
    "\n",
    "We don't care about:\n",
    "- what \"sky\" or \"blue\" mean\n",
    "- that this sentence conveys something about the weather\n",
    "\n",
    "These meanings arise in the mind of the receiver and involve interpretation, which is outside the scope of Information Theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5a42d-14be-482d-809b-3ff09a05d192",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "Information Theory would treat both \"The sky is blue\" and some random characters \"ajksd873##@\" in much the same way: as strings of characters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a1b8fa-3d5d-4479-b59a-3859bd9bbbc2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "The focus is on the quantity of information rather than its content or interpretation. In fact, the random string is considered as having more information content because it is less predictable.\n",
    "\n",
    "At its core, it is the mathematics of inference (encoding, decoding, probability, and statistics) especially under conditions of uncertainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf59aa-0501-4c0f-a792-84b4bc85644c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "Shannon’s key insight was that messages aren’t random strings though. Some symbols/words are more probable than others. e.g.:\n",
    "- In English text, E is much more likely than Z.\n",
    "- The digram TH is far more likely than XZ.\n",
    "- Whole words like “the” occur with very high frequency.\n",
    "\n",
    "So instead of assuming equal probabilities (each symbol would carry $\\log_2 27 \\approx 4.75$ bits), he measured (or estimated) them from language statistics. \n",
    "\n",
    "The actual entropy rate is only $\\approx 1–1.5$ bits/letter. This “gap” is *redundancy* in language, which is why compression and error correction are possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801fafe5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "## Entropy \n",
    "Because information is the reduction of uncertainty associated with observing data, we must first understand uncertainty.\n",
    "\n",
    "This is done with entropy: a mathematical function that quantifies how uncertain we are (on average) about the state of some variable *before* any message is received.\n",
    "\n",
    "There are many ways to define entropy; we will focus on the perspective of entropy as expected surprise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0237528-32b8-4c4f-b2a8-0b8760a76ef2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "Note that we are not discussing entropy from physics/thermodynamics, which you are likely familiar with.\n",
    "\n",
    "Both measure the logarithm of possible states — but our emphasis is on information and complexity, not heat and energy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603c82d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "### Entropy as expected surprise\n",
    "\n",
    "Some events are more surprising than others.\n",
    "\n",
    "How surprised you are depends on how unlikely the event was."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9521c4-d768-486f-9706-392e9637664b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "Consider a fair die:\n",
    "- each face has probability of 1/6\n",
    "- no outcome is more surprising than another (if you were gambling on the die: no reason to pick any face over another)\n",
    "\n",
    "![](images/Die.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e5700-82dc-4ab1-885e-74fc52160d1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "Now consider a loaded die: \n",
    "- suppose 2 comes up 2/3 of the time, all other faces come up with probability 1/15\n",
    "- knowing this probability distribution ahead of time, you will be less surprised to see a 2 come up than you would be to see a 5 come up.\n",
    "\n",
    "![](images/Die_Roman.jpg)\n",
    "\n",
    "So how do we quantify surprise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41a6ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "We want a function for the surprise of an event $h(x)$, that satisfies: \n",
    "1. more probable events are less surprising ($h(x)\\propto 1/P(x)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c547b8-9958-4cc2-9d7e-2652fbda17ed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "2. surprise is never be negative "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4366bc-9fba-4bdf-8965-9ad1c3b3393b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "3. independent events are additive ($h(x_1,x_2)=h(x_1)+h(x_2)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1079f2bb-96f7-4f82-8121-aece496aea3f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "4. surprise varies continuously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9121a-94f4-420f-b487-8e0a1c59ae21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "5. guaranteed events are unsurprising: $P(x) = 1 \\implies h(x) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113789a-67c4-4632-afb4-16eb02ce8a6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "6. impossible events are infinitely surprising: $P(x)=0 \\implies h(x)=\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67663f5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "The unique function meeting these criteria is:\n",
    "\n",
    "$$\n",
    "h(x)=-\\log P(x).\n",
    "$$\n",
    "\n",
    "This is called **surprise**, **self-information** or the **local entropy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e1ec9-647b-43af-a541-d21bde66becc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "Having defined the surprise of a single outcome, we now ask: *What is the average surprise if we observe the system many times?*\n",
    "\n",
    "To answer this, we take the **expected value of surprise** across all possible outcomes. \n",
    "\n",
    "This is the **Shannon entropy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa5643",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "##### Recall: Expected value\n",
    "\n",
    "The **expected value** of a random variable is computed by taking the sum of all possible outcomes, each weighted by its probability \n",
    "\n",
    "For a discrete random variable $X$, the expected value $\\mathbb{E}[X]$ is given by:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\sum_{x} p(x) \\cdot x\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $p(x)$ is the probability of outcome $x$\n",
    "- $x$ is the value of the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ad5e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "### Shannon Entropy, $H(X)$\n",
    "Formally, entropy is the expected surprise over all outcomes:\n",
    "\n",
    "$$\n",
    "H(X)=\\mathbb{E}_X[h(X)]=-\\sum_{x\\in \\mathcal{X}}P(x)\\log P(x).\n",
    "$$\n",
    "\n",
    "Key ideas:\n",
    "- rare events are very surprising but we aren't that surprised very often\n",
    "- frequent events are unsurprising, so we spend a lot of time not being very surprised.\n",
    "- entropy balances these, giving the average uncertainty. \n",
    "\n",
    "Note that by convention we say that $0 \\log(0) = 0$, so impossible events have no impact on our uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee319394-590d-44b0-858c-2a2cff2ae167",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "Shannon entropy is *the* fundamental building block of Information Theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d99328-c058-415b-8d28-e489a25188b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "So which dice - the fair or the loaded one - has a higher entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d7cf29-f01a-44d3-b0a1-c54b5326e5d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "**Fair die**: is maximally uncertain — all outcomes equally likely.\n",
    "\n",
    "$$\n",
    "H(X) = -6\\cdot\\tfrac{1}{6}\\log_2\\tfrac{1}{6} = \\log_2 6 \\approx 2.59 \\text{ bits.}\n",
    "$$\n",
    "\n",
    "**Loaded die** ($P(2)=\\tfrac{2}{3},\\; P(\\text{others})=\\tfrac{1}{15}$): is more predictable (most of the time it’s “2”), so the average uncertainty is lower\n",
    "\n",
    "$$\n",
    "H(X) = -\\Big[\\tfrac{2}{3}\\log_2\\tfrac{2}{3} + 5\\cdot\\tfrac{1}{15}\\log_2\\tfrac{1}{15}\\Big] \\approx 1.69 \\text{ bits.}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9ac14c-f3c5-4b52-bc5f-da071ef8ce5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "<div style=\"border-left: 4px solid #1e70bf; padding: 0.75em; background-color: #eaf3fb; margin-bottom: 1em;\">\n",
    "  <strong>Test your understanding:</strong><br>\n",
    "  Does rolling a die or tossing a coin have a higher entropy?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b10b6-3278-410d-8ada-2bd582019331",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "```{toggle}\n",
    "Rolling a die has higher entropy than tossing a coin.\n",
    "    \n",
    "Each outcome of a die toss has smaller probability ($p=1/6$) than each outcome of a coin toss ($p=1/2).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b4dce1-9ff8-49c1-ba65-965ec825b96c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "You might like to keep this picture for a fair coin toss in mind:\n",
    "\n",
    "For a Bernoulli random variable with probabilities $p$ and $1-p$:\n",
    "\n",
    "$$\n",
    "H(p) = -p \\log p - (1-p) \\log (1-p).\n",
    "$$\n",
    "\n",
    "![](images/Binary_entropy_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8a5df-d63d-4d92-b6cd-b23c8c264edf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "Note that $-x \\log x$ (single-term contribution) is asymmetric, peaks at $1/e$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bfba15-c418-45b6-8509-e1fb02975390",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "In general entropy is a high-dimensional surface that lives on the *probability simplex*, which is the set of all valid probability distributions.\n",
    "\n",
    "**Details**:\n",
    "\n",
    "For a system with $k$ possible outcomes, we describe it with probabilities\n",
    "\n",
    "$$\n",
    "(p_1, p_2, \\dots, p_k), \\quad \\sum_{i=1}^k p_i = 1, \\quad p_i \\geq 0.\n",
    "$$\n",
    "\n",
    "- For a coin ($k=2$): the simplex is 1D (a line segment from $(1,0)$ to $(0,1)$) (the parabola-like binary entropy).\n",
    "- For a die ($k=6$): the simplex is 5D.\n",
    "\n",
    "Entropy is defined by all $k$ probabilities together.:\n",
    "\n",
    "$$\n",
    "H(p_1,\\dots,p_k) = -\\sum_{i=1}^k p_i \\log p_i.\n",
    "$$\n",
    "\n",
    "- This is a **surface** defined over the simplex.\n",
    "- At one corner of the simplex (all probability on a single outcome), entropy = 0.\n",
    "- At the centre of the simplex (all probabilities equal), entropy = $\\log k$, the maximum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6897ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "### Examples\n",
    "\n",
    "Consider some known distributions...\n",
    "\n",
    "Let $X$ be a normally distributed random variable, $X \\sim N(\\mu, \\sigma^2)$, with probability density function (PDF) $f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(X) &= -\\int_{-\\infty}^{\\infty} f(x) \\log f(x) \\, dx \\\\\n",
    "\\therefore H_{\\text{normal}} &= -\\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) \\log \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\right) \\, dx \\\\\n",
    "&= -\\int_{-\\infty}^{\\infty} f(x) \\left(-\\frac{1}{2} \\log(2 \\pi \\sigma^2) - \\frac{(x - \\mu)^2}{2\\sigma^2}\\right) \\, dx\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The first term (the constant $\\frac{1}{2} \\log(2 \\pi \\sigma^2) $) comes out of the integral and integrates to 1, and the second term integrates to give $\\sigma^2 $. The result is:\n",
    "\n",
    "$$\n",
    "H_{\\text{normal}} = \\frac{1}{2} \\log(2 \\pi e \\sigma^2)\n",
    "$$\n",
    "\n",
    "Thus, the entropy of a normal distribution depends on the variance $\\sigma^2 $. The larger the variance, the more spread out the distribution, and thus the higher the entropy.\n",
    "\n",
    "Let $X$ be an exponentially distributed random variable, $X \\sim \\text{Exp}(\\lambda) $ with probability density function (PDF), $f(x) = \\lambda \\exp(-\\lambda x) \\quad \\text{for} \\, x \\geq 0$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(X) &= -\\int_{0}^{\\infty} f(x) \\log f(x) \\, dx \\\\\n",
    "\\therefore H_{\\text{exponential}} &= -\\int_{0}^{\\infty} \\lambda \\exp(-\\lambda x) \\log(\\lambda \\exp(-\\lambda x)) \\, dx \\\\\n",
    "&= -\\int_{0}^{\\infty} \\lambda \\exp(-\\lambda x) \\left(\\log(\\lambda) - \\lambda x\\right) \\, dx\\\\\n",
    "&= -\\log(\\lambda) \\int_{0}^{\\infty} \\lambda \\exp(-\\lambda x) \\, dx + \\lambda \\int_{0}^{\\infty} \\lambda x \\exp(-\\lambda x) \\, dx\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The first integral evaluates to 1, and the second integral evaluates to $\\frac{1}{\\lambda}$. Hence,\n",
    "\n",
    "$$\n",
    "H_{\\text{exponential}} = -\\log(\\lambda) + 1\n",
    "$$\n",
    "\n",
    "Thus, the entropy of an exponential distribution depends on the rate parameter $\\lambda$. The smaller the rate, the more spread out the distribution, and thus the higher the entropy.\n",
    "\n",
    "#### Comparing\n",
    "The normal distribution's entropy grows with the variance $\\sigma^2$, while the exponential distribution's entropy is inversely related to the rate $\\lambda$\n",
    "\n",
    "For comparable scales (a similar spread/variance):\n",
    "\n",
    "Normal distribution $N(\\mu, \\sigma^2)$: \n",
    "- $H_{\\text{normal}} = \\frac{1}{2} \\log(2 \\pi e \\sigma^2)$\n",
    "- defined over the entire real line and spreads out more\n",
    "- generally has higher entropy\n",
    "\n",
    "Exponential distribution $\\text{Exp}(\\lambda)$: \n",
    "- (Recall that the variance of the exponential distribution is $\\frac{1}{\\lambda^2}$.)\n",
    "- $H_{\\text{exponential}} = 1 - \\log(\\lambda)$\n",
    "- despite its unbounded support, concentrates more heavily near 0 and decays rapidly\n",
    "- generally has lower entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de8b9f4-8bbd-4d19-9296-3718dd74b7c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "<div style=\"border-left: 4px solid #1e70bf; padding: 0.75em; background-color: #eaf3fb; margin-bottom: 1em;\">\n",
    "  <strong>Pause and consider:</strong><br>\n",
    "Order the behavioural regimes of a Couzin model in terms of their entropy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e44b91-927a-4190-b8f3-b609c8462fa9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "outputs": [],
   "source": [
    "```{toggle}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7559f61-3f16-447d-aac4-6ec766f746ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "### Summarising\n",
    "\n",
    "The question we set out to answer was \"WHAT is complexity\"\n",
    "\n",
    "We have answered a different question: How uncertain is the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277147c-cf37-4c1a-a9db-e6b31f04c5a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "#### Absolute meaning\n",
    "\n",
    "Entropy is measured in **bits (or nats)**, and it tells you:\n",
    "- the *average surprise per symbol*, or\n",
    "- the *average number of yes/no questions* needed to identify an outcome.\n",
    "\n",
    "So if you calculate $H(X) = 2.59$ bits for a fair die, that literally means: on average, you need $\\approx 2.59$ binary questions to specify the face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b98886-8c8e-4686-9384-67014e450333",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "#### Relative meaning\n",
    "\n",
    "On its own, the *magnitude* of entropy is less useful unless you compare it to something, like:\n",
    "- **Maximum entropy**: For $k$ equally likely outcomes, the maximum possible entropy is $\\log_2 k$.\n",
    "- **Normalisation**: You often look at entropy relative to this maximum (so you can say a distribution is “60% of maximum uncertainty”).\n",
    "- **Comparisons across systems**: Entropy lets you compare two models, two distributions or even to the same system under different conditions, in a principled way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f1f4a-efe8-4e6d-92bb-a9751f476ac2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "#### Interpretation in practice\n",
    "\n",
    "- **low** $H$: outcomes are predictable, the system has order.\n",
    "- **high** $H$: outcomes are uncertain, the system is closer to randomness.\n",
    "- If you just quote “$H=1.7$ bits,” the number alone is meaningless unless you also know: *“out of how many possible outcomes?”*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9a48f5-f0df-43cb-9f65-e86d73c99dcb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "Entropy gives us a rigorous measure of uncertainty — the average surprise in outcomes — but that is not the same as a measure of complexity. Instead, it provides the toolkit we need to measure the uncertainty, dependencies, and information flows that underlie complex behaviour.\n",
    "\n",
    "And so unfortunately we don't have a value for complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73394031-51ac-4d06-a853-5efe85590479",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "By extending entropy to compare variables (through conditional entropy, joint entropy, and mutual information) we can begin to capture relationships and structure that move us closer to the idea of complexity...\n",
    "\n",
    "These are essential ingredients of complex systems: emergence, adaptation, and interaction all hinge on flows of information and uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e7a63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "## Joint entropy, $H(X,Y)$\n",
    "\n",
    "Measures the total uncertainty in two random variables considered together.\n",
    "- Reflects the uncertainty of each variables *and* their interactions.\n",
    "- Useful for understanding interdependencies: higher dependence means lower joint entropy (relative to the independent case).\n",
    "\n",
    "Formally defined as the union of both marginal entropies:\n",
    "\n",
    "$$\n",
    "H(X,Y)=\\mathbb{E}_{X,Y}[h(X,Y)]=-\\sum_{x\\in \\mathcal{X}\\\\ y\\in \\mathcal{Y}}P(x,y)\\log P(x,y).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fbb450-fde3-4432-a67a-abbfcca2b574",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "Joint and marginal entropies are generally related by:\n",
    "$$\n",
    "H(X,Y)\\leq H(X)+H(Y)\n",
    "$$\n",
    "with equality if and only if the variables are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf6c72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "## Conditional entropy, $H(X|Y)$\n",
    "\n",
    "Quantifies how much uncertainty remains in $X$ once the state of $Y$ is known.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\n",
    "H(X|Y)=\\mathbb{E}_{X,Y}[h(X|Y)]=-\\sum_{x\\in \\mathcal{X}\\\\ y\\in \\mathcal{Y}}P(x,y)\\log P(x|y).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd51e832-5f05-440b-92eb-dff0bb1f6434",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "\n",
    "Note that we have broken away from the familiar “$p\\log p$” formula here and the expectation is taken with respect to the joint distribution $P(x,y)$, since both variables contribute to the average uncertainty. Specifically:\n",
    "- the **log term** uses the **conditional probability** $P(x|y)$, because we’re asking about the distribution of $X$ given $Y$.\n",
    "- but the **weights** in the sum must still reflect how often each $(x,y)$ pair actually occurs and so the expectation is taken with respect to the **joint distribution** $P(x,y)$.\n",
    "- (if we only summed over $P(x|y)$, we’d be treating all values of $y$ as equally likely. The joint distribution $P(x,y)$ ensures the average is **weighted correctly** by how often each conditioning case occurs.)\n",
    "\n",
    "For a particular, fixed state $y$:\n",
    "\n",
    "$$\n",
    "H(X|y) = \\mathbb{E}_{X|y}[h(X|Y)] = -\\sum_x P(x|y)\\,\\log P(x|y),\n",
    "$$\n",
    "\n",
    "which measures the uncertainty in $X$ when $Y=y$ is known."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452b666",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Mutual information, $I(X; Y)$\n",
    "\n",
    "A measure of **statistical dependency** between two random variables.\n",
    "\n",
    "Answers: *how much does knowing $X$ reduce uncertainty about $Y$ (and vice versa)?*\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\n",
    "I(X;Y) = \\sum_{x,y} P(x,y)\\,\\log \\frac{P(x,y)}{P(x)P(y)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041ae34-992f-40cc-b8a2-cc9baf02a1b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "\n",
    "Key properties:\n",
    "- $I(X;Y) = 0$: $X$ and $Y$ are independent (knowing one tells you nothing about the other).\n",
    "- $I(X;Y)$ is always non-negative.\n",
    "- Maximum $I(X;Y)$ occurs when one variable completely determines the other, in which case:\n",
    "\n",
    "$$\n",
    "I(X;Y) = H(X) = H(Y).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f55732-2149-4caa-997d-500af570c2c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "Recall: \n",
    "- $P(x), P(y)$ are the marginal probabilities of $X$ and $Y$ respectively\n",
    "- $P(x,y)$ is the **joint probability distribution**: the probability that variable $X$ takes value $x$ and variable $Y$ takes value $y$ simultaneously.\n",
    "- $P(x|y)$ is the **conditional probability** of $X=x$ given that $Y=y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f48d6ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Many others too\n",
    "e.g. Relative entropy (Kullback-Liebler divergence)\n",
    "\n",
    "...\n",
    "\n",
    "We won't get into the details here but you may come across them when researching for your Project and are welcome to use (or invent) any sensible information measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9313b8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "# Putting it all together\n",
    "\n",
    "![](images/Entropy_VennDiagram.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fff718c-0a37-4977-a896-ecf4412691a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "\n",
    "The area of each circle corresponds to the amount of uncertainty we have about the state of each variable (*not* the amount of information we have, which is a common misinterpretation).\n",
    "\n",
    "(And these relationships generalise for more variables.)\n",
    "\n",
    "Hence we can think about mutual information in terms of the aforementioned entropies:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(X; Y) &= H(X) - H(X|Y) \\\\\n",
    "&= H(X) + H(Y) - H(X, Y) \\\\\n",
    "&= H(Y) - H(Y|X) \\\\\n",
    "&= H(X,Y) - H(Y|X) - H(X|Y)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efb326",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Applications\n",
    "\n",
    "So far, our examples (like dice) assumed we knew the outcome probabilities in advance. In practice, we often do not — but **information-theoretic quantities can still be computed directly from data**.\n",
    "\n",
    "Entropy has the general form:\n",
    "\n",
    "$$\n",
    "H = -\\sum p \\log p,\n",
    "$$\n",
    "\n",
    "so once we can estimate the probabilities $p$ from observations, we can calculate entropy and related measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e2f638-423e-4688-9734-5516725278b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "## Time series analysis\n",
    "\n",
    "Examples in time-series analysis include:\n",
    "- **Permutation entropy**: quantifies the diversity of ordinal patterns in a sequence.\n",
    "- **Transfer entropy**: captures directional information flow between time series.\n",
    "\n",
    "Note that these extensions are not assessable in the Tests but are included as an example of how you might extend these ideas for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4296894",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Permutation Entropy\n",
    "\n",
    "Captures permutation patterns from a scalar time series measured from a complex system by organising windows of the time series with length $n$, according to their *relative values* forming the symbol $\\pi_i$:.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\n",
    "H(n) = -\\sum_i^{n!} p(\\pi_i)\\log(p(\\pi_i)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13d153-5c9e-4dd8-9d49-6c8889a0f92f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "The simple example provided by Bandt and Pompe...\n",
    "\n",
    "- For $x=(4,7,9,10,6,11,3)$ and $n=2$\n",
    "    - we find find four pairs for which $x_{t+1}>x_{t}$ and two pairs for which $x_{t+1}<x_{t}$, i.e.\n",
    "    - 4/6 pairs are represented by the permutation 01 (an increase)\n",
    "    - 2/6 are represented by 10 (a decrease)\n",
    "    - permutation entropy (a measure of the probabilities of the permutations 01 and 10):\n",
    "      \n",
    "$$\n",
    "H(2) = -\\frac{2}{6}\\log\\left(\\frac{2}{6}\\right) - \\frac{4}{6}\\log\\left(\\frac{4}{6}\\right) \\approx 0.918\n",
    "$$\n",
    "\n",
    "<!-- Bandt, Pompe, Phys Rev Lett. 2002 Apr 29;88(17):174102.  doi: 10.1103/PhysRevLett.88.174102 -->\n",
    "\n",
    "Everything you wanted to know about permutation entropy (and ordinal methods) is [here](https://pubs.aip.org/collection/1214/Ordinal-Methods-Concepts-Applications-New?utm_[…]s%20and%20Challenges_Topical_2023&dm_i=1XPS,8DVWE,5T8ELS,YL4WO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e0803f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "![](images/PermutationEntropy.png)\n",
    "\n",
    "Image from Ordpy package intro [here](https://doi.org/10.48550/arXiv.2102.06786).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b18443",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Transfer Entropy \n",
    "\n",
    "Based on conditional mutual information $I(X; Y |Z)$ of the variables $X$, $Y$ given the variable $Z$:\n",
    "\n",
    "$$\n",
    "I(X; Y |Z) = H(X|Z) + H(Y |Z) - H(X, Y |Z)\n",
    "$$\n",
    "\n",
    "Schreiber introduced transfer entropy in 2000 as a means to detect asymmetries in interaction of two coupled dynamical systems from their time series.\n",
    "\n",
    "\n",
    "It measures how much knowledge of the present state of $X$ reduces uncertainty about the future of $Y$, beyond what is already explained by $Y$'s own past:\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\n",
    "T_{X\\rightarrow Y} = I(X_t, Y_{t+1}|Y_t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee13a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In the context of animal interactions, $X$ and $Y$ will measure the behavior of two individuals and transfer entropy will be constructed based on raw time series extracted from behavioral observations or trajectory tracking:\n",
    "\n",
    "![](images/TransferEntropy_Fish.png)\n",
    "\n",
    "Image from [here](https://doi.org/10.1007/s11721-018-0157-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023d264-fd84-42df-bb4b-66a351ed1e95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "reader-only"
    ]
   },
   "source": [
    "Transfer entropy here provides insights into the phase transition. At the critical point, the transfer entropy exhibits unique behaviour, indicating a shift in the complexity of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a058e5c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In the context of financial markets Korbel et al. analysed information flow between various market communities. They focussed on information transfer of rare events (typically large drops which can spread in the network):\n",
    "\n",
    "![](images/TransferEntropy_StockExchange.png)\n",
    "\n",
    "Image from [here](https://doi.org/10.3390/e21111124)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad48d03-fe37-4f2f-92fa-021aab955926",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "slides"
    ]
   },
   "source": [
    "## Data in general\n",
    "\n",
    "Not all entropy-based extensions are tied to time series. Many are designed for static data, distributions or networks.\n",
    "\n",
    "Fundamentally though, the process is the same:\n",
    "1. Define/extract a probability distribution\n",
    "2. Compute $-\\sum p\\log p$ or a suitable variant of it\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
